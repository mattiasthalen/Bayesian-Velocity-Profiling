{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9dMSQcQ5Lb3"
      },
      "source": [
        "# Bayesian Velocity Profiling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DihdfgnGBWuo"
      },
      "source": [
        "## Setup Environment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Libraries"
      ],
      "metadata": {
        "id": "l-vn_F0mmmG2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmfYY3UIaKxZ",
        "outputId": "96470be8-de5b-4111-e451-c70edb6891ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pymc==4.1.4 in /usr/local/lib/python3.7/dist-packages (4.1.4)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from pymc==4.1.4) (1.3.5)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from pymc==4.1.4) (1.7.3)\n",
            "Requirement already satisfied: fastprogress>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from pymc==4.1.4) (1.0.3)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from pymc==4.1.4) (1.5.0)\n",
            "Requirement already satisfied: aeppl==0.0.33 in /usr/local/lib/python3.7/dist-packages (from pymc==4.1.4) (0.0.33)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from pymc==4.1.4) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from pymc==4.1.4) (4.1.1)\n",
            "Requirement already satisfied: aesara==2.7.9 in /usr/local/lib/python3.7/dist-packages (from pymc==4.1.4) (2.7.9)\n",
            "Requirement already satisfied: arviz>=0.12.0 in /usr/local/lib/python3.7/dist-packages (from pymc==4.1.4) (0.12.1)\n",
            "Requirement already satisfied: cachetools>=4.2.1 in /usr/local/lib/python3.7/dist-packages (from pymc==4.1.4) (4.2.4)\n",
            "Requirement already satisfied: setuptools>=45.0.0 in /usr/local/lib/python3.7/dist-packages (from aesara==2.7.9->pymc==4.1.4) (57.4.0)\n",
            "Requirement already satisfied: etuples in /usr/local/lib/python3.7/dist-packages (from aesara==2.7.9->pymc==4.1.4) (0.3.5)\n",
            "Requirement already satisfied: cons in /usr/local/lib/python3.7/dist-packages (from aesara==2.7.9->pymc==4.1.4) (0.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from aesara==2.7.9->pymc==4.1.4) (3.8.0)\n",
            "Requirement already satisfied: miniKanren in /usr/local/lib/python3.7/dist-packages (from aesara==2.7.9->pymc==4.1.4) (1.0.3)\n",
            "Requirement already satisfied: logical-unification in /usr/local/lib/python3.7/dist-packages (from aesara==2.7.9->pymc==4.1.4) (0.4.5)\n",
            "Requirement already satisfied: xarray>=0.16.1 in /usr/local/lib/python3.7/dist-packages (from arviz>=0.12.0->pymc==4.1.4) (0.20.2)\n",
            "Requirement already satisfied: xarray-einstats>=0.2 in /usr/local/lib/python3.7/dist-packages (from arviz>=0.12.0->pymc==4.1.4) (0.2.2)\n",
            "Requirement already satisfied: netcdf4 in /usr/local/lib/python3.7/dist-packages (from arviz>=0.12.0->pymc==4.1.4) (1.6.0)\n",
            "Requirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.7/dist-packages (from arviz>=0.12.0->pymc==4.1.4) (3.2.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from arviz>=0.12.0->pymc==4.1.4) (21.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0->arviz>=0.12.0->pymc==4.1.4) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0->arviz>=0.12.0->pymc==4.1.4) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0->arviz>=0.12.0->pymc==4.1.4) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0->arviz>=0.12.0->pymc==4.1.4) (1.4.4)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->pymc==4.1.4) (2022.2.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=3.0->arviz>=0.12.0->pymc==4.1.4) (1.15.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from xarray>=0.16.1->arviz>=0.12.0->pymc==4.1.4) (4.12.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.7/dist-packages (from logical-unification->aesara==2.7.9->pymc==4.1.4) (0.12.0)\n",
            "Requirement already satisfied: multipledispatch in /usr/local/lib/python3.7/dist-packages (from logical-unification->aesara==2.7.9->pymc==4.1.4) (0.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->xarray>=0.16.1->arviz>=0.12.0->pymc==4.1.4) (3.8.1)\n",
            "Requirement already satisfied: cftime in /usr/local/lib/python3.7/dist-packages (from netcdf4->arviz>=0.12.0->pymc==4.1.4) (1.6.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
            "Collecting jax[cuda11_cudnn82]==0.3.15\n",
            "  Downloading jax-0.3.15.tar.gz (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 28.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from jax[cuda11_cudnn82]==0.3.15) (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.7/dist-packages (from jax[cuda11_cudnn82]==0.3.15) (1.21.6)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.7/dist-packages (from jax[cuda11_cudnn82]==0.3.15) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.7/dist-packages (from jax[cuda11_cudnn82]==0.3.15) (1.7.3)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.7/dist-packages (from jax[cuda11_cudnn82]==0.3.15) (4.1.1)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.7/dist-packages (from jax[cuda11_cudnn82]==0.3.15) (0.7.1)\n",
            "Collecting jaxlib==0.3.15+cuda11.cudnn82\n",
            "  Downloading https://storage.googleapis.com/jax-releases/cuda11/jaxlib-0.3.15%2Bcuda11.cudnn82-cp37-none-manylinux2014_x86_64.whl (162.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 162.8 MB 25 kB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp in /usr/local/lib/python3.7/dist-packages (from etils[epath]->jax[cuda11_cudnn82]==0.3.15) (3.8.1)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.7/dist-packages (from etils[epath]->jax[cuda11_cudnn82]==0.3.15) (5.9.0)\n",
            "Building wheels for collected packages: jax\n",
            "  Building wheel for jax (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jax: filename=jax-0.3.15-py3-none-any.whl size=1201920 sha256=bf9fac460ad4d510d282fb0c7ec1bdff6a2c27e06742f433bc015a3dbdad4ee2\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/36/dc/90243142efbc28986784e072b123d4e674dc79edd013ce665b\n",
            "Successfully built jax\n",
            "Installing collected packages: jaxlib, jax\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.3.15+cuda11.cudnn805\n",
            "    Uninstalling jaxlib-0.3.15+cuda11.cudnn805:\n",
            "      Successfully uninstalled jaxlib-0.3.15+cuda11.cudnn805\n",
            "  Attempting uninstall: jax\n",
            "    Found existing installation: jax 0.3.17\n",
            "    Uninstalling jax-0.3.17:\n",
            "      Successfully uninstalled jax-0.3.17\n",
            "Successfully installed jax-0.3.15 jaxlib-0.3.15+cuda11.cudnn82\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting numpyro==0.10.1\n",
            "  Downloading numpyro-0.10.1-py3-none-any.whl (292 kB)\n",
            "\u001b[K     |████████████████████████████████| 292 kB 27.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: multipledispatch in /usr/local/lib/python3.7/dist-packages (from numpyro==0.10.1) (0.6.0)\n",
            "Requirement already satisfied: jax>=0.2.13 in /usr/local/lib/python3.7/dist-packages (from numpyro==0.10.1) (0.3.15)\n",
            "Requirement already satisfied: jaxlib>=0.1.65 in /usr/local/lib/python3.7/dist-packages (from numpyro==0.10.1) (0.3.15+cuda11.cudnn82)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from numpyro==0.10.1) (4.64.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from numpyro==0.10.1) (1.21.6)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.7/dist-packages (from jax>=0.2.13->numpyro==0.10.1) (0.7.1)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.7/dist-packages (from jax>=0.2.13->numpyro==0.10.1) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jax>=0.2.13->numpyro==0.10.1) (4.1.1)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.7/dist-packages (from jax>=0.2.13->numpyro==0.10.1) (1.7.3)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from jax>=0.2.13->numpyro==0.10.1) (1.2.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.7/dist-packages (from etils[epath]->jax>=0.2.13->numpyro==0.10.1) (5.9.0)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.7/dist-packages (from etils[epath]->jax>=0.2.13->numpyro==0.10.1) (3.8.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from multipledispatch->numpyro==0.10.1) (1.15.0)\n",
            "Installing collected packages: numpyro\n",
            "Successfully installed numpyro-0.10.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting blackjax==0.8.3\n",
            "  Downloading blackjax-0.8.3-py3-none-any.whl (106 kB)\n",
            "\u001b[K     |████████████████████████████████| 106 kB 35.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jax>=0.3.13 in /usr/local/lib/python3.7/dist-packages (from blackjax==0.8.3) (0.3.15)\n",
            "Requirement already satisfied: jaxlib>=0.3.10 in /usr/local/lib/python3.7/dist-packages (from blackjax==0.8.3) (0.3.15+cuda11.cudnn82)\n",
            "Collecting jaxopt>=0.4.2\n",
            "  Downloading jaxopt-0.5-py3-none-any.whl (128 kB)\n",
            "\u001b[K     |████████████████████████████████| 128 kB 75.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: fastprogress>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from blackjax==0.8.3) (1.0.3)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.7/dist-packages (from jax>=0.3.13->blackjax==0.8.3) (0.7.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from jax>=0.3.13->blackjax==0.8.3) (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.7/dist-packages (from jax>=0.3.13->blackjax==0.8.3) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jax>=0.3.13->blackjax==0.8.3) (4.1.1)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.7/dist-packages (from jax>=0.3.13->blackjax==0.8.3) (1.7.3)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.7/dist-packages (from jax>=0.3.13->blackjax==0.8.3) (3.3.0)\n",
            "Requirement already satisfied: matplotlib>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from jaxopt>=0.4.2->blackjax==0.8.3) (3.2.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.1->jaxopt>=0.4.2->blackjax==0.8.3) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.1->jaxopt>=0.4.2->blackjax==0.8.3) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.1->jaxopt>=0.4.2->blackjax==0.8.3) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.1->jaxopt>=0.4.2->blackjax==0.8.3) (1.4.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=2.0.1->jaxopt>=0.4.2->blackjax==0.8.3) (1.15.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.7/dist-packages (from etils[epath]->jax>=0.3.13->blackjax==0.8.3) (5.9.0)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.7/dist-packages (from etils[epath]->jax>=0.3.13->blackjax==0.8.3) (3.8.1)\n",
            "Installing collected packages: jaxopt, blackjax\n",
            "Successfully installed blackjax-0.8.3 jaxopt-0.5\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement pint_xarray==0.18 (from versions: 0.1, 0.2, 0.2.1)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for pint_xarray==0.18\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting bottleneck==1.3.5\n",
            "  Downloading Bottleneck-1.3.5-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (355 kB)\n",
            "\u001b[K     |████████████████████████████████| 355 kB 27.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from bottleneck==1.3.5) (1.21.6)\n",
            "Installing collected packages: bottleneck\n",
            "Successfully installed bottleneck-1.3.5\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting numbagg==0.2.1\n",
            "  Downloading numbagg-0.2.1-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from numbagg==0.2.1) (1.21.6)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (from numbagg==0.2.1) (0.56.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba->numbagg==0.2.1) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.7/dist-packages (from numba->numbagg==0.2.1) (0.39.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from numba->numbagg==0.2.1) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba->numbagg==0.2.1) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba->numbagg==0.2.1) (4.1.1)\n",
            "Installing collected packages: numbagg\n",
            "Successfully installed numbagg-0.2.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ipython-autotime==0.3.1\n",
            "  Downloading ipython_autotime-0.3.1-py2.py3-none-any.whl (6.8 kB)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from ipython-autotime==0.3.1) (7.9.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime==0.3.1) (4.4.2)\n",
            "Collecting jedi>=0.10\n",
            "  Downloading jedi-0.18.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 24.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime==0.3.1) (2.6.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime==0.3.1) (5.1.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime==0.3.1) (0.2.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime==0.3.1) (2.0.10)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime==0.3.1) (0.7.5)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime==0.3.1) (4.8.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime==0.3.1) (57.4.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.10->ipython->ipython-autotime==0.3.1) (0.8.3)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->ipython-autotime==0.3.1) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->ipython-autotime==0.3.1) (0.2.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->ipython-autotime==0.3.1) (0.7.0)\n",
            "Installing collected packages: jedi, ipython-autotime\n",
            "Successfully installed ipython-autotime-0.3.1 jedi-0.18.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pymc==4.1.4\n",
        "!pip install \"jax[cuda11_cudnn82]==0.3.15\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
        "!pip install numpyro==0.10.1\n",
        "!pip install blackjax==0.8.3\n",
        "!pip install pint_xarray==0.2.1\n",
        "!pip install bottleneck==1.3.5\n",
        "!pip install numbagg==0.2.1\n",
        "!pip install ipython-autotime==0.3.1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup Platform"
      ],
      "metadata": {
        "id": "WwuQ_oVXgYW1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import jax.tools.colab_tpu\n",
        "    import numpyro.util\n",
        "    \n",
        "    jax.tools.colab_tpu.setup_tpu()\n",
        "    numpyro.util.set_platform('tpu')\n",
        "\n",
        "    print('TPU assigned.')\n",
        "\n",
        "    import jax\n",
        "    display(jax.local_devices())\n",
        "\n",
        "except:\n",
        "    print('No TPU available, trying GPU.')\n",
        "    \n",
        "    try:\n",
        "        numpyro.set_platform('gpu')\n",
        "        \n",
        "        print('GPU assigned.')\n",
        "\n",
        "        import jax\n",
        "        display(jax.local_devices())\n",
        "\n",
        "\n",
        "    except:\n",
        "        numpyro.set_platform('cpu')\n",
        "\n",
        "        print('No GPU available, using CPU.')\n",
        "\n",
        "        import jax\n",
        "        display(jax.local_devices())\n",
        "        \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "qNXwSpAYe-Y0",
        "outputId": "260aa1f9-b538-4cac-e78a-2288ad00c22a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No TPU available, trying GPU.\n",
            "GPU assigned.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[GpuDevice(id=0, process_index=0)]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVaqZX-3OTWl"
      },
      "source": [
        "### Load Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9R2YmMNmPQFn",
        "outputId": "c1bf220f-23d6-446c-bf9d-9541acb52274"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 328 µs (started: 2022-09-07 11:24:10 +00:00)\n"
          ]
        }
      ],
      "source": [
        "%load_ext autotime"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pymc as pm\n",
        "import pymc.sampling_jax"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohaxLYsbf4e2",
        "outputId": "c2aec378-c2ff-4ebd-9fd6-054b36849cbd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 5.74 s (started: 2022-09-07 11:24:10 +00:00)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pymc/sampling_jax.py:36: UserWarning: This module is experimental.\n",
            "  warnings.warn(\"This module is experimental.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "R43yCeSDP5BD",
        "outputId": "e0b8dd79-5722-41ab-c80b-618676f490d1"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-7380678735e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpint_xarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pint_xarray'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 213 ms (started: 2022-09-07 11:24:16 +00:00)\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import pint_xarray\n",
        "import warnings\n",
        "\n",
        "import requests\n",
        "import numpyro\n",
        "import sklearn.preprocessing\n",
        "import sklearn.metrics\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import arviz as az\n",
        "import xarray as xr\n",
        "\n",
        "from jax import numpy as jnp\n",
        "from numpyro import distributions as dist\n",
        "\n",
        "from collections import OrderedDict\n",
        "from google.colab import drive\n",
        "from requests import get\n",
        "from urllib.parse import unquote\n",
        "\n",
        "from requests import get\n",
        "from urllib.parse import unquote\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from typing import Callable, Optional, Dict, List, Union, NoReturn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PQgvlBLOjTS"
      },
      "source": [
        "### Set Global Vars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q315qyvjA7lm"
      },
      "outputs": [],
      "source": [
        "rng_key = jax.random.PRNGKey(33)\n",
        "n_devices = len(jax.local_devices())\n",
        "numpyro.set_host_device_count(n_devices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hCopJyB9oteF"
      },
      "outputs": [],
      "source": [
        "plt.style.use('bmh')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3Rq7AHHnJNT"
      },
      "outputs": [],
      "source": [
        "csv_path = 'https://raw.githubusercontent.com/mattiasthalen/Bayesian-Velocity-Profiling/main/RepOne_Data_Export.csv'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blDhXZLZQ4BB"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXNFV7da7HqZ"
      },
      "source": [
        "### Regression Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8cBkPiSVrBW"
      },
      "outputs": [],
      "source": [
        "def calc_obs_weight(mvt, velocity, weight, max_weight):\n",
        "    try:\n",
        "        mvt = mvt.pint.dequantify()\n",
        "        velocity = velocity.pint.dequantify()\n",
        "        weight = weight.pint.dequantify()\n",
        "        max_weight = max_weight.pint.dequantify()\n",
        "    except:\n",
        "        pass\n",
        "    finally:\n",
        "        obs_weight = 0.5**((1 - mvt/velocity)/0.2) * 0.5**((1 - weight/max_weight)/0.2)\n",
        "        \n",
        "    return obs_weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uB8AOtcGnbkj"
      },
      "outputs": [],
      "source": [
        "def linear_fn(x, intercept, slope):\n",
        "    return intercept + slope*x\n",
        "\n",
        "def linear_rev_fn(x, intercept, slope):\n",
        "    return (x - intercept)/slope\n",
        "\n",
        "def linear_fit(ds, var, coord, reduce_dims):\n",
        "    ds = ds.where((ds.coords['set_type'] != 'Back Off') & (ds['workup_sets'] > 2), drop = True)\\\n",
        "              .sel({'aggregation': 'max'}, drop = True)\\\n",
        "              .pint.dequantify()\n",
        "\n",
        "    coefs = ds[var].curvefit(coords = ds[coord],\n",
        "                             func = linear_fn,\n",
        "                             reduce_dims = reduce_dims)\\\n",
        "                   .rename({'param': 'regression_param'})\n",
        "    \n",
        "    return coefs['curvefit_coefficients']\n",
        "\n",
        "def linear_predict(x, coefs, reverse = False):\n",
        "    intercept = coefs.sel(regression_param = 'intercept', drop = True)\n",
        "    slope = coefs.sel(regression_param = 'slope', drop = True)\n",
        "\n",
        "    if reverse:\n",
        "            return linear_rev_fn(x, intercept, slope)\n",
        "\n",
        "    return linear_fn(x, intercept, slope)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdWQyE4gQ8BF"
      },
      "source": [
        "### Plotting Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMHAdbMtRBdk"
      },
      "outputs": [],
      "source": [
        "def plot_pbc(ds, exercise, data_var, window = 20, signal_window = 8, ax = None):\n",
        "    df = ds[data_var].sel({'exercise': exercise}, drop = True)\\\n",
        "                     .drop_vars(['training_cycle', 'cycle_type', 'max_weight_pr_flag'])\\\n",
        "                     .to_dataframe()\\\n",
        "                     .dropna()\n",
        "    \n",
        "    df['moving_average'] = df[data_var].sort_index(ascending = False)\\\n",
        "                                       .rolling(window, min_periods = 1)\\\n",
        "                                       .mean()\n",
        "    \n",
        "    df['moving_range'] = df[data_var].diff(-1)\\\n",
        "                                     .abs()\\\n",
        "                                     .sort_index(ascending = False)\\\n",
        "                                     .rolling(window, min_periods = 1)\\\n",
        "                                     .mean()\n",
        "\n",
        "    df['process_average'] = df['moving_average']\n",
        "    df['process_range'] = df['moving_range']\n",
        "    df['signal'] = None\n",
        "    df['signal_min'] = None\n",
        "    df['signal_max'] = None\n",
        "    df['signal_above_average'] = None\n",
        "    df['signal_below_average'] = None\n",
        "\n",
        "    n_rows = len(df)\n",
        "    previous_signal_id = 0\n",
        "\n",
        "    for row in np.arange(n_rows):\n",
        "        first_row = row == 0\n",
        "        sufficient_rows_left = n_rows - row >= window\n",
        "\n",
        "        signal_start_id = np.max([8, row - signal_window])\n",
        "\n",
        "        df['signal_min'][row] = df[data_var][signal_start_id:row].min()\n",
        "        df['signal_max'][row] = df[data_var][signal_start_id:row].max()\n",
        "\n",
        "        df['signal_above_average'][row] = (df['signal_min'][row] > df['process_average'][row - 1])\n",
        "        df['signal_below_average'][row] = (df['signal_max'][row] < df['process_average'][row - 1])\n",
        "\n",
        "        signal_open = (first_row) | (row >= previous_signal_id + window)\n",
        "        signal = (signal_open) & (sufficient_rows_left) & (first_row | df['signal_above_average'][row] | df['signal_below_average'][row])\n",
        "        df['signal'][row] = signal\n",
        "        \n",
        "        df['process_average'][row] =  df['process_average'][row - 1]\n",
        "        df['process_range'][row] =  df['process_range'][row - 1]\n",
        "\n",
        "        if signal:\n",
        "            previous_signal_id = row\n",
        "            df['process_average'][row] =  df['moving_average'][row]\n",
        "            df['process_range'][row] =  df['moving_range'][row]\n",
        "        else:\n",
        "            df['process_average'][row] =  df['process_average'][row - 1]\n",
        "            df['process_range'][row] =  df['process_range'][row - 1]\n",
        "\n",
        "    df['lower_limit_1'] = df['process_average'] - df['process_range']/1.128\n",
        "    df['upper_limit_1'] = df['process_average'] + df['process_range']/1.128\n",
        "    df['lower_limit_2'] = df['process_average'] - df['process_range']*2/1.128\n",
        "    df['upper_limit_2'] = df['process_average'] + df['process_range']*2/1.128\n",
        "    df['lower_limit_3'] = df['process_average'] - df['process_range']*3/1.128\n",
        "    df['upper_limit_3'] = df['process_average'] + df['process_range']*3/1.128\n",
        "\n",
        "    if ax is None:\n",
        "        ax = plt.gca()\n",
        "\n",
        "    ax.scatter(df.index, df[data_var], marker = '.', alpha = 0.6)\n",
        "    ax.plot(df.index, df['process_average'])\n",
        "    ax.plot(df.index, df['lower_limit_3'])\n",
        "    ax.plot(df.index, df['upper_limit_3'])\n",
        "\n",
        "    ax.fill_between(df.index,df['lower_limit_1'], df['upper_limit_1'], alpha = 0.3)\n",
        "\n",
        "    ax.set_xlabel('')\n",
        "    ax.set_ylabel('')\n",
        "    #ax.set_ylim([0, None])\n",
        "    ax.tick_params(labelrotation = 90)\n",
        "    ax.grid()\n",
        "\n",
        "    return ax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rTUijXu0i27"
      },
      "outputs": [],
      "source": [
        "def plot_kpis(ds, exercise, vars):\n",
        "    var_titles = [var.title().replace('_', ' ').replace('1Rm', '1RM') for var in vars]\n",
        "\n",
        "    n_vars = len(vars)\n",
        "\n",
        "    n_cols = 1\n",
        "\n",
        "    if n_vars > 1:\n",
        "        n_cols = 2\n",
        "    \n",
        "    n_rows = 1\n",
        "\n",
        "    if n_vars > 2:\n",
        "        n_rows = np.ceil(n_vars/n_cols).astype(int)\n",
        "    \n",
        "    figsize = np.array([6, 3]) * [n_cols, n_rows]\n",
        "\n",
        "    fig, axes = plt.subplots(ncols = n_cols,\n",
        "                            nrows = n_rows,\n",
        "                            constrained_layout = True,\n",
        "                            figsize = figsize,\n",
        "                            sharex = True)\n",
        "    \n",
        "    axes = [ax for row in axes for ax in row]\n",
        "\n",
        "    for key, val in enumerate(vars):\n",
        "        plot_pbc(ds, exercise, vars[key], ax = axes[key])\n",
        "        title = var_titles[key]\n",
        "        axes[key].set_title(title)\n",
        "\n",
        "    fig.suptitle(f'{exercise.title()} KPIs', fontsize = 16)\n",
        "    fig.supxlabel('Workout Start Time')\n",
        "\n",
        "    plt.draw()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubofKRwqoci-"
      },
      "source": [
        "### ETL Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qgN4QogIocFZ"
      },
      "outputs": [],
      "source": [
        "def new_extract(csv_path, ds_path, **kwargs):\n",
        "    try:\n",
        "        return os.path.getmtime(csv_path) > os.path.getmtime(ds_path)\n",
        "    except:\n",
        "        return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nyqfPoReqF_Y"
      },
      "outputs": [],
      "source": [
        "def extract_data(csv_path, **kwargs):\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    df.columns = (df.columns\n",
        "                    .str.lower()\n",
        "                    .str.replace(' \\(m/s\\)', '')\n",
        "                    .str.replace(' \\(mm\\)', '')\n",
        "                    .str.replace(' \\(sec\\)', '')\n",
        "                    .str.replace(' \\(%\\)', '')\n",
        "                    .str.replace(' ', '_'))\n",
        "    \n",
        "    df.rename(columns = {'weight': 'load'}, inplace = True)\n",
        "\n",
        "    df['workout_start_time'] = pd.to_datetime(df['workout_start_time'], format = '%d/%m/%Y, %H:%M:%S')\n",
        "\n",
        "    df.dropna(subset = ['exercise'], inplace = True)\n",
        "    df['rest_time'] = pd.to_timedelta(df['rest_time'])\n",
        "\n",
        "    # Correct split session\n",
        "    df['set'].mask((df['exercise'] == 'deadlift') & (df['workout_start_time'] == pd.to_datetime('2020-12-30 13:06:04')), df['set'] + 7, inplace = True)\n",
        "    df.replace({'workout_start_time': pd.to_datetime('2020-12-30 13:06:04')}, pd.to_datetime('2020-12-30 12:53:09'), inplace = True)\n",
        "    df.replace({'workout_start_time': pd.to_datetime('2021-01-07 11:50:22')}, pd.to_datetime('2021-01-07 11:20:07'), inplace = True)\n",
        "    df.replace({'workout_start_time': pd.to_datetime('2021-06-10 12:02:22')}, pd.to_datetime('2021-06-10 11:56:31'), inplace = True)\n",
        "    df.replace({'workout_start_time': pd.to_datetime('2021-06-14 12:06:00')}, pd.to_datetime('2021-06-14 11:57:50'), inplace = True)\n",
        "\n",
        "    # Reindex sets & reps to counter bugs in the extract\n",
        "    df['set'] = df.groupby(['exercise', 'workout_start_time'])['set'].apply(lambda x: (x != x.shift()).cumsum() - 1)\n",
        "    df['rep'] = df.groupby(['exercise', 'workout_start_time', 'set']).cumcount()\n",
        "\n",
        "    # Convert from , to . as decimal sign\n",
        "    df['load'] = df['load'].str.replace(',', '.').astype('float')\n",
        "\n",
        "    # Drop rows with tag fail\n",
        "    fail_filter = df['tags'].str.contains('fail', na = False)\n",
        "    df = df[~fail_filter]\n",
        "\n",
        "    # Handle the case when a rep is split into two reps\n",
        "    rep_split_filter = df['tags'].str.contains('rep split', na = False)\n",
        "\n",
        "    rep_split_df = df[rep_split_filter].groupby(['exercise', 'workout_start_time', 'set', 'load', 'metric'])[['range_of_motion', 'duration_of_rep']].sum()\n",
        "    rep_split_df['avg_velocity'] = rep_split_df['range_of_motion']/1000/rep_split_df['duration_of_rep']\n",
        "    rep_split_df['rep'] = 0\n",
        "    rep_split_df.reset_index(inplace = True)\n",
        "\n",
        "    rep_split_df = rep_split_df.groupby(['exercise', 'workout_start_time', 'set', 'rep']).max()\n",
        "\n",
        "    df = pd.concat([df[~rep_split_filter], rep_split_df])\n",
        "\n",
        "    # Group to get multi index\n",
        "    df = df.groupby(['exercise', 'workout_start_time', 'set', 'rep']).max()\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xz8urMMhrcIV"
      },
      "outputs": [],
      "source": [
        "def define_cycles(cycles, min_workout_start_time, **kwargs):\n",
        "    cycle_start_dates = np.array(list(cycles.keys())).astype('datetime64[ns]')\n",
        "\n",
        "    if min_workout_start_time < cycle_start_dates.min():\n",
        "        cycles = {min_workout_start_time.strftime('%Y-%m-%d'): 'n/a', **cycles}\n",
        "        cycle_start_dates = np.array(list(cycles.keys())).astype('datetime64[ns]')\n",
        "\n",
        "    cycle_types = np.array(list(cycles.values()))\n",
        "    cycle_id = np.arange(len(cycle_start_dates))\n",
        "\n",
        "    ds = xr.Dataset(coords = {'workout_start_time': cycle_start_dates,\n",
        "                              'training_cycle': ('workout_start_time', cycle_id),\n",
        "                              'cycle_type': ('workout_start_time', cycle_types)})\n",
        "\n",
        "    return ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kdOFZqLRtlZp"
      },
      "outputs": [],
      "source": [
        "def transform_data(df, **kwargs):\n",
        "    # Convert to xarray\n",
        "    ds = df.to_xarray()\n",
        "\n",
        "    # Change Set and Rep to integers\n",
        "    ds['set'] = ds['set'].astype(int)\n",
        "    ds['rep'] = ds['rep'].astype(int)\n",
        "\n",
        "    # Move variables to coords\n",
        "    ds = ds.set_coords(['metric', 'tags'])\n",
        "\n",
        "    # Define UOMs\n",
        "    ds = ds.pint.quantify({'load': 'kg',\n",
        "                           'avg_velocity': 'meter / second',\n",
        "                           'peak_velocity': 'meter / seconds',\n",
        "                           'range_of_motion': 'mm',\n",
        "                           'duration_of_rep': 's'})\n",
        "\n",
        "    # Session meta data\n",
        "    session_stack = ['exercise', 'workout_start_time']\n",
        "    ds['session_max_load'] = ds['load'].stack(stack = session_stack)\\\n",
        "                                       .groupby('stack')\\\n",
        "                                       .reduce(all_nan_max, ...)\\\n",
        "                                       .unstack()\n",
        "\n",
        "    # Set meta data\n",
        "    set_stack = ['exercise', 'workout_start_time', 'set']\n",
        "    ds['load'] = (ds['load'].stack(stack = set_stack)\n",
        "                            .groupby('stack')\n",
        "                            .reduce(all_nan_max, ...)\n",
        "                            .unstack())\n",
        "\n",
        "    ds['reps'] = (ds['avg_velocity'].stack(stack = set_stack)\n",
        "                                    .groupby('stack')\n",
        "                                    .count(...)\n",
        "                                    .unstack()\n",
        "                                    .where(ds['load'] > 0, drop = True))\n",
        "\n",
        "    ds['set_velocities'] = summarize(ds['avg_velocity'].pint.dequantify())\n",
        "    ds['set_velocities'] = ds['set_velocities'].pint.quantify({ds['set_velocities'].name: 'mps'})\n",
        "\n",
        "    ds.coords['set_type'] = assign_set_type(ds['load'])\n",
        "\n",
        "    # Merge training cycle coordinates\n",
        "    cycles = define_cycles(**kwargs)\n",
        "    ds = ds.merge(cycles, join = 'outer')\n",
        "    ds['cycle_type'] = ('workout_start_time', ds['cycle_type'].to_series().ffill())\n",
        "    ds['training_cycle'] = ds['training_cycle'].ffill('workout_start_time').astype(int)\n",
        "\n",
        "    # Add the running min top set velocity per exercise\n",
        "    ds['minimum_velocity_threshold'] = (ds['set_velocities'].sel({'aggregation': 'first'})\n",
        "                                                            .where(ds.coords['set_type'] == 'Top Set')\n",
        "                                                            .pint.dequantify()\n",
        "                                                            .stack(stack = ['exercise', 'workout_start_time'])\n",
        "                                                            .groupby('stack')\n",
        "                                                            .reduce(all_nan_min, ...)\n",
        "                                                            .unstack()\n",
        "                                                            .rolling({'workout_start_time': len(ds['workout_start_time'])},\n",
        "                                                                    min_periods = 1)\n",
        "                                                            .min())\n",
        "    ds['minimum_velocity_threshold'] = ds['minimum_velocity_threshold'].pint.quantify({ds['minimum_velocity_threshold'].name: 'meter / second'})\n",
        "\n",
        "    # Add running max load per exercise\n",
        "    ds['rolling_max_load'] = (ds['load'].pint.dequantify()\n",
        "                                             .stack(stack = ['exercise', 'workout_start_time'])\n",
        "                                             .groupby('stack')\n",
        "                                             .reduce(all_nan_max, ...)\n",
        "                                             .unstack()\n",
        "                                             .rolling({'workout_start_time': len(ds['workout_start_time'])},\n",
        "                                                      min_periods = 1)\n",
        "                                             .max())\n",
        "    ds['rolling_max_load'] = ds['rolling_max_load'].pint.quantify({ds['rolling_max_load'].name: 'kg'})\n",
        "\n",
        "    # Generate the observation weights\n",
        "    ds['observation_weight'] = calc_obs_weight(ds['minimum_velocity_threshold'], ds['set_velocities'], ds['load'], ds['session_max_load'])\n",
        "\n",
        "    # Additional session meta data\n",
        "    ds['workup_sets'] = ds['load'].where(ds.coords['set_type'] == 'Work Up', drop = True)\\\n",
        "                                  .stack(stack = session_stack)\\\n",
        "                                  .groupby('stack')\\\n",
        "                                  .count(...)\\\n",
        "                                  .unstack()\n",
        "    \n",
        "    ds['session_regression_coefficients'] = linear_fit(ds, 'load', 'set_velocities', 'set')\n",
        "\n",
        "    ds['estimated_1rm'] = linear_predict(ds['minimum_velocity_threshold'].pint.dequantify(), ds['session_regression_coefficients'])\n",
        "    ds['estimated_1rm'] = ds['estimated_1rm'].pint.quantify({ds['estimated_1rm'].name: 'kg'})\n",
        "\n",
        "    ds['zero_velocity_load'] = linear_predict(0, ds['session_regression_coefficients'])\n",
        "    ds['zero_velocity_load'] = ds['zero_velocity_load'].pint.quantify({ds['zero_velocity_load'].name: 'kg'})\n",
        "\n",
        "    ds['zero_load_velocity'] = linear_predict(0, ds['session_regression_coefficients'], reverse = True)\n",
        "    ds['zero_load_velocity'] = ds['zero_load_velocity'].pint.quantify({ds['zero_load_velocity'].name: 'mps'})\n",
        "\n",
        "    ds['curve_score'] = ds['zero_velocity_load'].pint.dequantify()*ds['zero_load_velocity'].pint.dequantify()/2\n",
        "\n",
        "    ds['session_volume'] = (ds['load'] * ds['reps']).stack(stack = session_stack).groupby('stack').sum(...).unstack()\n",
        "    ds['session_relative_volume'] = ds['session_volume']/ds['estimated_1rm']\n",
        "\n",
        "    # Rep meta data\n",
        "    ds['rep_exertion'] = linear_predict(ds['avg_velocity'].pint.dequantify(), ds['session_regression_coefficients'])/ds['estimated_1rm'].pint.dequantify()\n",
        "    ds['rep_force'] = (ds['load']*ds['range_of_motion'].pint.to('meter')/ds['duration_of_rep']**2).pint.to('N')\n",
        "    ds['rep_energy'] = (ds['rep_force']*ds['range_of_motion'].pint.to('meter')).pint.to('J')\n",
        "\n",
        "    # Session meta data\n",
        "    ds['session_exertion_load'] = ds['rep_exertion'].stack(stack = ['exercise', 'workout_start_time']).groupby('stack').reduce(all_nan_sum, ...).unstack().pint.dequantify()\n",
        "\n",
        "    # Add PR coordinates\n",
        "    ds.coords['max_load_pr_flag'] = ds['rolling_max_load'].diff('workout_start_time') > 0\n",
        "\n",
        "    return ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVUixL-apqjH"
      },
      "outputs": [],
      "source": [
        "def etl_data(**kwargs):\n",
        "    df = extract_data(**kwargs)\n",
        "\n",
        "    min_workout_start_time = df.index.get_level_values('workout_start_time').min().floor('D')\n",
        "\n",
        "    ds = transform_data(df, min_workout_start_time = min_workout_start_time, **kwargs)\n",
        "\n",
        "    return ds\n",
        "    \n",
        "    try:\n",
        "        os.remove(ds_path)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    try:\n",
        "        ds.to_netcdf(ds_path)\n",
        "    except PermissionError:\n",
        "        print('PermissionError')    \n",
        "\n",
        "    return ds   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONhxw8JIpB9A"
      },
      "outputs": [],
      "source": [
        "def get_data(**kwargs):\n",
        "    #if not new_extract(**kwargs):\n",
        "    #    return xr.open_dataset(kwargs['ds_path'])\n",
        "    \n",
        "    return etl_data(**kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_PgwhIy7mF0"
      },
      "source": [
        "### Miscellaneous Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tnyih5CgK4EY"
      },
      "outputs": [],
      "source": [
        "def assign_set_type(da):\n",
        "    set_category = xr.where(da['set'] < da.idxmax('set'), 'Work Up', np.nan)\n",
        "    set_category = xr.where(da['set'] == da.idxmax('set'), 'Top Set', set_category)\n",
        "    set_category = xr.where(da['set'] > da.idxmax('set'), 'Back Off', set_category)\n",
        "    set_category = xr.where(np.isnan(da), np.nan, set_category)\n",
        "    \n",
        "    return set_category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FV50KZquxdj5"
      },
      "outputs": [],
      "source": [
        "def agg_summarize(x):\n",
        "    x = x[np.isfinite(x)]\n",
        "\n",
        "    if len(x) > 0:\n",
        "        min = np.min(x)\n",
        "        max = np.max(x)\n",
        "        first = x[0]\n",
        "        last = x[-1]\n",
        "        peak_end = np.mean([min, last])\n",
        "        mean = np.mean(x)\n",
        "        median = np.median(x)\n",
        "        hdi = az.hdi(x)\n",
        "        result = np.array([min, max, first, last, peak_end, mean, median, *hdi])\n",
        "    else:\n",
        "        result = np.array([np.nan]*9)\n",
        "    \n",
        "    return result\n",
        "\n",
        "def summarize(x, reduce_dim = 'rep'):\n",
        "    summaries = xr.apply_ufunc(agg_summarize,\n",
        "                               x,\n",
        "                               vectorize = True,\n",
        "                               input_core_dims = [[reduce_dim]],\n",
        "                               output_core_dims = [['aggregation']])\n",
        "    \n",
        "    summaries['aggregation'] = ['min', 'max', 'first', 'last', 'peak_end', 'mean', 'median', 'hdi_lower', 'hdi_upper']\n",
        "    \n",
        "    return summaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTGyzLdT0s5G"
      },
      "outputs": [],
      "source": [
        "def agg_hdi_summary(x):\n",
        "    mean = x.mean()\n",
        "    median = np.median(x)\n",
        "    hdi = az.hdi(x)\n",
        "\n",
        "    return np.array([mean, median, *hdi])\n",
        "\n",
        "def hdi_summary(x, reduce_dim = 'sample'):\n",
        "    try:\n",
        "        x = x.pint.dequantify()\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    summaries = xr.apply_ufunc(agg_hdi_summary,\n",
        "                               x,\n",
        "                               vectorize = True,\n",
        "                               input_core_dims = [[reduce_dim]],\n",
        "                               output_core_dims = [['hdi_aggregation']])\n",
        "    \n",
        "    summaries['hdi_aggregation'] = ['mean', 'median', 'hdi_lower', 'hdi_upper']\n",
        "    \n",
        "    return summaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gisKtoreeVSI"
      },
      "outputs": [],
      "source": [
        "def all_nan_summary(x, mode = 'mean', **kwargs):\n",
        "    if np.all(np.isnan(x)):\n",
        "        return np.nan\n",
        "    elif mode == 'max':\n",
        "        return np.nanmax(x)\n",
        "    elif mode == 'min':\n",
        "        return np.nanmin(x)\n",
        "    elif mode == 'mean':\n",
        "        return np.nanmean(x)\n",
        "    elif mode == 'sum':\n",
        "        return np.nansum(x)\n",
        "        \n",
        "def all_nan_max(x, **kwargs):\n",
        "    return all_nan_summary(x, 'max', **kwargs)\n",
        "        \n",
        "def all_nan_min(x, **kwargs):\n",
        "    return all_nan_summary(x, 'min', **kwargs)\n",
        "        \n",
        "def all_nan_mean(x, **kwargs):\n",
        "    return all_nan_summary(x, 'mean', **kwargs)\n",
        "        \n",
        "def all_nan_sum(x, **kwargs):\n",
        "    return all_nan_summary(x, 'sum', **kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqmRu_8pOLt4"
      },
      "outputs": [],
      "source": [
        "def profile(data, exercise, ds, mvt = None, plot = True, use_weights = True, **kwargs):\n",
        "    tic = time.perf_counter()\n",
        "\n",
        "    data = np.array(list(data.items()))\n",
        "    \n",
        "    load = data[:, 0]\n",
        "    velocity = data[:, 1]\n",
        "    \n",
        "    # Set minimum velocity threshold\n",
        "    if mvt is None:        \n",
        "        try:\n",
        "            mvt = ds['minimum_velocity_threshold'].sel({'exercise': exercise.lower()})[-1]\n",
        "        except:\n",
        "            mvt = mvt.mean()\n",
        "\n",
        "        try:\n",
        "            mvt = mvt.pint.dequantify().item()\n",
        "        except:\n",
        "            mvt = mvt.item()\n",
        "    \n",
        "    mvt = np.nanmin([mvt, velocity.min()])\n",
        "\n",
        "    # Max load\n",
        "    try:\n",
        "        max_load = ds['rolling_max_weight'].sel({'exercise': exercise.lower()})[-1]\n",
        "        try:\n",
        "            max_load = max_load.pint.dequantify().item()\n",
        "        except:\n",
        "            max_load = max_load.item()\n",
        "    except:\n",
        "        max_load = np.nan\n",
        "    \n",
        "    max_load = np.nanmax([max_load, load.max()])\n",
        "    \n",
        "    if use_weights:\n",
        "        weight = calc_obs_weight(mvt, velocity, load, max_load)\n",
        "    else:\n",
        "        weight = np.ones(len(load))\n",
        "\n",
        "    #inference_data = quadratic_fit(load, velocity, weight, **kwargs)\n",
        "\n",
        "    #if plot:\n",
        "    #    plot_profile(inference_data, mvt, exercise, **kwargs)\n",
        "\n",
        "    toc = time.perf_counter()\n",
        "    print(f'Profile completed in {toc - tic:0.4f} seconds.')\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBrZ4RuhqIdC"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SovWMw2MtZR6"
      },
      "outputs": [],
      "source": [
        "cycles = {'2021-01-05': '95% x1 + 75% x6+ @ 90%',\n",
        "          '2021-01-27': '95% x1',\n",
        "          '2021-02-01': '95% x1 + 75% x3+ @ 80%',\n",
        "          '2021-02-15': '95% x1 + 80% x1 @ 85%',\n",
        "          '2021-03-15': '95% x1 + 75% x6[3] @ 90%',\n",
        "          '2021-03-24': '95% x1 + 75% x3 @ 90%',\n",
        "          '2021-04-10': '95% x1 + 75% x2-3 @ 90-95% [320]',\n",
        "          '2021-04-13': '95% x1 + 80% x2 @ 95% [320]',\n",
        "          '2021-04-20': '95% x1',\n",
        "          '2021-06-01': '95% x1 + 85% x2',\n",
        "          '2021-07-06': 'YOLO',\n",
        "          }\n",
        "\n",
        "data = get_data(csv_path = csv_path,\n",
        "              cycles = cycles)\n",
        "\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8G37MnsGlipD"
      },
      "source": [
        "### Prepare Data For Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pR5KAgvkBCs4"
      },
      "outputs": [],
      "source": [
        "regr_data = (data[['load', 'set_velocities', 'observation_weight']]\n",
        "             .pint.dequantify()\n",
        "             .sel({'aggregation': 'max'}, drop = True)\n",
        "             .where(data.coords['set_type'] != 'Back Off')\n",
        "             .where(data.coords['exercise'] != 'front squat')\n",
        "             .drop_vars(['set_type', 'training_cycle', 'max_load_pr_flag'])\n",
        "             .to_dataframe()\n",
        "             .dropna()\n",
        "             .reset_index()\n",
        "             .sort_values(by = ['workout_start_time', 'exercise', 'set'])\n",
        "             .rename(columns = {'set_velocities': 'velocity'}))\n",
        "\n",
        "load_scaler = sklearn.preprocessing.StandardScaler()\n",
        "regr_data['load_scaled'] = load_scaler.fit_transform(regr_data['load'].values.reshape(-1, 1))\n",
        "\n",
        "velocity_scaler = sklearn.preprocessing.StandardScaler()\n",
        "regr_data['velocity_scaled'] = velocity_scaler.fit_transform(regr_data['velocity'].values.reshape(-1, 1))\n",
        "\n",
        "exercise_encoder = sklearn.preprocessing.LabelEncoder()\n",
        "regr_data['exercise_id'] = exercise_encoder.fit_transform(regr_data['exercise'].values)\n",
        "\n",
        "regr_data['session_id'] = regr_data.groupby(['exercise', 'workout_start_time'], sort = False).ngroup()\n",
        "\n",
        "regr_data['observation'] = np.arange(len(regr_data))\n",
        "regr_data.set_index('observation', inplace = True)\n",
        "\n",
        "regr_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G21clwu5uzFn"
      },
      "outputs": [],
      "source": [
        "map_session_to_exercise_id = (regr_data[['session_id', 'exercise_id']]\n",
        "                              .drop_duplicates()\n",
        "                              .set_index('session_id', verify_integrity = True)\n",
        "                              .sort_index()['exercise_id']\n",
        "                              .values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "807i_I2KxLcB"
      },
      "outputs": [],
      "source": [
        "data_dict = dict({\n",
        "    'velocity_std': regr_data['velocity_scaled'].values,\n",
        "    'load_std': regr_data['load_scaled'].values,\n",
        "    'session_exercise_id': map_session_to_exercise_id,\n",
        "    'session_id': regr_data['session_id'].values\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnHjXkqWqMEV"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Model"
      ],
      "metadata": {
        "id": "2a0di4jclkci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(velocity_scaled,\n",
        "                load_scaled,\n",
        "                session_exercise_id,\n",
        "                session_id,\n",
        "                coords,\n",
        "                render_model = True):\n",
        "    \n",
        "    with pm.Model(coords = coords) as model:\n",
        "        # Inputs\n",
        "        velocity_shared = pm.Data('velocity_scaled', velocity_scaled, mutable = True, dims = 'observation')\n",
        "        session_exercise_id = pm.Data('session_exercise_id', session_exercise_id, mutable = True, dims = 'session')\n",
        "        session_id = pm.Data('session_id', session_id, mutable = True,  dims = 'observation')\n",
        "\n",
        "        # Global Parameters\n",
        "        intercept_mu_global = pm.Normal(name = 'intercept_mu_global',\n",
        "                                        mu = 0.0,\n",
        "                                        sigma = 1.0)\n",
        "        \n",
        "        intercept_sigma_global = pm.HalfNormal(name = 'intercept_sigma_global',\n",
        "                                            sigma = 1.0)\n",
        "        \n",
        "        slope_mu_global = pm.Gamma(name = 'slope_mu_global',\n",
        "                                alpha = 2.0,\n",
        "                                beta = 1.0)\n",
        "        \n",
        "        slope_sigma_global = pm.HalfNormal(name = 'slope_sigma_global',\n",
        "                                        sigma = 1.0)\n",
        "        \n",
        "        curve_mu_global = pm.HalfNormal(name = 'curve_mu_global',\n",
        "                                        sigma = 1.0)\n",
        "        \n",
        "        error_mu_global = pm.HalfNormal(name = 'error_mu_global',\n",
        "                                        sigma = 1.0)\n",
        "        \n",
        "        # Exercise Parameters\n",
        "        intercept_offset_exercise = pm.Normal(name = 'intercept_offset_exercise',\n",
        "                                            mu = 0.0,\n",
        "                                            sigma = 1.0,\n",
        "                                            dims = 'exercise')\n",
        "        \n",
        "        intercept_mu_exercise = pm.Deterministic(name = 'intercept_mu_exercise',\n",
        "                                                var = intercept_mu_global + intercept_sigma_global*intercept_offset_exercise,\n",
        "                                                dims = 'exercise')\n",
        "        \n",
        "        intercept_sigma_exercise = pm.HalfNormal(name = 'intercept_sigma_exercise',\n",
        "                                                sigma = 1.0,\n",
        "                                                dims = 'exercise')\n",
        "        \n",
        "        slope_offset_exercise = pm.Gamma(name = 'slope_offset_exercise',\n",
        "                                        alpha = 2.0,\n",
        "                                        beta = 1.0,\n",
        "                                        dims = 'exercise')\n",
        "        \n",
        "        slope_mu_exercise = pm.Deterministic(name = 'slope_mu_exercise',\n",
        "                                            var = slope_mu_global + slope_sigma_global*slope_offset_exercise,\n",
        "                                            dims = 'exercise')\n",
        "        \n",
        "        '''\n",
        "        slope_mu_exercise = pm.Gamma(name = 'slope_mu_exercise',\n",
        "                                    mu = slope_mu_global,\n",
        "                                    sigma = slope_sigma_global,\n",
        "                                    dims = 'exercise')\n",
        "        '''\n",
        "\n",
        "        slope_sigma_exercise = pm.HalfNormal(name = 'slope_sigma_exercise',\n",
        "                                            sigma = 1.0,\n",
        "                                            dims = 'exercise')\n",
        "        \n",
        "        curve_mu_exercise = pm.HalfNormal(name = 'curve_mu_exercise',\n",
        "                                        sigma = 1.0/curve_mu_global,\n",
        "                                        dims = 'exercise')\n",
        "        \n",
        "        '''\n",
        "        error_mu_exercise = pm.HalfNormal(name = 'error_mu_exercise',\n",
        "                                        sigma = 1.0/error_mu_global,\n",
        "                                        dims = 'exercise')\n",
        "        '''\n",
        "        \n",
        "        # Session Parameters\n",
        "        intercept_offset_session = pm.Normal(name = 'intercept_offset_session',\n",
        "                                            mu = 0.0,\n",
        "                                            sigma = 1.0,\n",
        "                                            dims = 'session')\n",
        "\n",
        "        intercept_mu_session = pm.Deterministic(name = 'intercept_mu_session',\n",
        "                                                var = (intercept_mu_exercise[session_exercise_id]\n",
        "                                                    + intercept_sigma_exercise[session_exercise_id]\n",
        "                                                    * intercept_offset_session[session_exercise_id]),\n",
        "                                                dims = 'session')\n",
        "\n",
        "        slope_offset_session = pm.Gamma(name = 'slope_offset_session',\n",
        "                                        alpha = 2.0,\n",
        "                                        beta = 1.0,\n",
        "                                        dims = 'session')\n",
        "        \n",
        "        slope_mu_session = pm.Deterministic(name = 'slope_mu_session',\n",
        "                                            var = (slope_mu_exercise[session_exercise_id]\n",
        "                                                + slope_sigma_exercise[session_exercise_id]\n",
        "                                                * slope_offset_session[session_exercise_id]),\n",
        "                                            dims = 'session')\n",
        "        \n",
        "        '''\n",
        "        slope_mu_session = pm.Gamma(name = 'slope_mu_session',\n",
        "                                    mu = slope_mu_exercise[session_exercise_id],\n",
        "                                    sigma = slope_sigma_exercise[session_exercise_id],\n",
        "                                    dims = 'session')\n",
        "        '''\n",
        "        \n",
        "        curve_mu_session = pm.HalfNormal(name = 'curve_mu_session',\n",
        "                                        sigma = 1.0/curve_mu_exercise[session_exercise_id],\n",
        "                                        dims = 'session')\n",
        "        \n",
        "        '''\n",
        "        error_mu_session = pm.HalfNormal(name = 'error_mu_session',\n",
        "                                        sigma = 1.0/error_mu_exercise[session_exercise_id],\n",
        "                                        dims = 'session')\n",
        "        '''\n",
        "\n",
        "        # Final Parameters\n",
        "        intercept = intercept_mu_session[session_id]\n",
        "        slope = slope_mu_session[session_id]\n",
        "        curve = curve_mu_session[session_id]\n",
        "        error = error_mu_global#error_mu_session[session_id]\n",
        "\n",
        "        # Estimated Value\n",
        "        load_mu = pm.Deterministic(name = 'load_mu',\n",
        "                                var = intercept - slope*velocity_shared - curve*velocity_shared**2,\n",
        "                                dims = 'observation')\n",
        "\n",
        "        # Likelihood\n",
        "        load_likelihood = pm.Normal(name = 'load_scaled',\n",
        "                                    mu = load_mu,\n",
        "                                    sigma = error,\n",
        "                                    observed = load_scaled,\n",
        "                                    dims = 'observation')\n",
        "    \n",
        "    if render_model:\n",
        "        display(pm.model_to_graphviz(model))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "wXAaBR2b7d_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "velocity_scaled = regr_data['velocity_scaled']\n",
        "load_scaled = regr_data['load_scaled']\n",
        "session_exercise_id = map_session_to_exercise_id\n",
        "session_id = regr_data['session_id']\n",
        "\n",
        "coords = {'observation': regr_data.index.values,\n",
        "          'exercise': exercise_encoder.classes_,\n",
        "          'session': regr_data['session_id'].unique()}\n",
        "\n",
        "quadratic_model = build_model(velocity_scaled, load_scaled, session_exercise_id, session_id, coords)"
      ],
      "metadata": {
        "id": "wfAEig5r7sUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sampling"
      ],
      "metadata": {
        "id": "Q3o9s9XjmYEs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Posterior"
      ],
      "metadata": {
        "id": "7CGCO2pRovMX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with pymc_model:\n",
        "    pymc_inference_data = pm.sampling_jax.sample_numpyro_nuts(draws = 2000,\n",
        "                                                              tune = 2000,\n",
        "                                                              chains = 1,\n",
        "                                                              target_accept = 0.98)\n",
        "\n",
        "pymc_inference_data"
      ],
      "metadata": {
        "id": "8BWLdd-5ClFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pymc_inference_data.extend(pm.sample_posterior_predictive(pymc_inference_data, model = pymc_model))"
      ],
      "metadata": {
        "id": "cJB7QBAVFg3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pymc_inference_data"
      ],
      "metadata": {
        "id": "o4Kf7c-4Fl7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_ = az.plot_trace(pymc_inference_data,\n",
        "                  compact = True,\n",
        "                  combined = True,\n",
        "                  figsize = [15, 20],\n",
        "                  var_names = ['intercept_mu_global', 'slope_mu_global', 'curve_mu_global', 'error_mu_global'])"
      ],
      "metadata": {
        "id": "kqXuVGp1fO1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_ = az.plot_trace(pymc_inference_data,\n",
        "                  compact = True,\n",
        "                  combined = True,\n",
        "                  legend = True,\n",
        "                  figsize = [15, 20],\n",
        "                  var_names = ['intercept_mu_exercise', 'slope_mu_exercise', 'curve_mu_exercise'])"
      ],
      "metadata": {
        "id": "mjC1ot7zHnM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_ = az.plot_trace(pymc_inference_data,\n",
        "                  compact = True,\n",
        "                  combined = True,\n",
        "                  figsize = [15, 20],\n",
        "                  var_names = ['intercept_mu_session', 'slope_mu_session', 'curve_mu_session'])"
      ],
      "metadata": {
        "id": "NPFs-CfuHtUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Prior"
      ],
      "metadata": {
        "id": "G63OVfGSoqsj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pymc_prior_predictive = pm.sample_prior_predictive(model = pymc_model)\n",
        "pymc_prior_predictive"
      ],
      "metadata": {
        "id": "481z32nenzKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YK2zkkNcJUS"
      },
      "source": [
        "## Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnLyjybwbGIc"
      },
      "source": [
        "### Generate Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5lMd1J9WQI8"
      },
      "outputs": [],
      "source": [
        "def create_prediction_df(unique_session_df,\n",
        "                         predict_velocity_series):\n",
        "    unique_session_df = unique_session_df.copy()\n",
        "    unique_session_df['_temp'] = True\n",
        "\n",
        "    predict_velocity_df = pd.DataFrame(predict_velocity_series, columns = ['velocity_scaled'])\n",
        "    predict_velocity_df['_temp'] = True\n",
        "\n",
        "    df = (unique_session_df\n",
        "          .merge(predict_velocity_df, on = ['_temp'])\n",
        "          .drop('_temp', axis = 1))\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjTX0B2vFsYz"
      },
      "outputs": [],
      "source": [
        "sessions = regr_data[['exercise', 'workout_start_time', 'exercise_id', 'session_id']].drop_duplicates()\n",
        "\n",
        "velocity_start = 0.0\n",
        "velocity_stop = np.ceil(regr_data['velocity'].max())\n",
        "velocity = np.arange(velocity_start, velocity_stop*100 + 1)/100\n",
        "velocity_scaled = velocity_scaler.transform(velocity.reshape(-1, 1)).flatten()\n",
        "\n",
        "predict_velocities = pd.Series(velocity_scaled)\n",
        "\n",
        "prediction_template = create_prediction_df(sessions, predict_velocities)\n",
        "prediction_template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0r_io7hQ0x7"
      },
      "outputs": [],
      "source": [
        "prediction_template = prediction_template.merge(regr_data[['session_id', 'velocity_scaled', 'load_scaled']],\n",
        "                                                how = 'left',\n",
        "                                                on = ['session_id', 'velocity_scaled'])\n",
        "\n",
        "prediction_template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMs2c9v4N1MR"
      },
      "outputs": [],
      "source": [
        "session_key, rng_key = jax.random.split(rng_key)\n",
        "\n",
        "session_ids = prediction_template['session_id'].values\n",
        "velocities = prediction_template['velocity_scaled'].values\n",
        "\n",
        "predictive = numpyro.infer.Predictive(quadratic_model,\n",
        "                                      posterior_samples = posterior_samples,\n",
        "                                      return_sites = ['load_std'])\n",
        "\n",
        "samples_predictive = predictive(session_key,\n",
        "                                velocity_std = velocities,\n",
        "                                session_exercise_id = map_session_to_exercise_id,\n",
        "                                session_id = session_ids)\n",
        "\n",
        "predictions = samples_predictive['load_std']\n",
        "median = jnp.median(predictions, axis = 0)\n",
        "lower, upper = numpyro.diagnostics.hpdi(predictions, axis = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vi-8EW7loRdx"
      },
      "outputs": [],
      "source": [
        "prediction_df = prediction_template.copy()\n",
        "\n",
        "# Add predictions\n",
        "prediction_df['load_scaled_median'] = median\n",
        "prediction_df['load_scaled_lower'] = lower\n",
        "prediction_df['load_scaled_upper'] = upper\n",
        "\n",
        "# Inverse scale all values\n",
        "prediction_df['velocity'] = velocity_scaler.inverse_transform(prediction_df['velocity_scaled'].values.reshape(-1, 1))\n",
        "prediction_df['load'] = load_scaler.inverse_transform(prediction_df['load_scaled'].values.reshape(-1, 1))\n",
        "prediction_df['load_median'] = load_scaler.inverse_transform(prediction_df['load_scaled_median'].values.reshape(-1, 1))\n",
        "prediction_df['load_lower'] = load_scaler.inverse_transform(prediction_df['load_scaled_lower'].values.reshape(-1, 1))\n",
        "prediction_df['load_upper'] = load_scaler.inverse_transform(prediction_df['load_scaled_upper'].values.reshape(-1, 1))\n",
        "\n",
        "prediction_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "078aLMX6Xn0F"
      },
      "outputs": [],
      "source": [
        "def plot_session(df, session_id, ax):\n",
        "    filter = (df['session_id'] == session_id) & (df['load_median'] >= 0)\n",
        "    data = df[filter]\n",
        "\n",
        "    auc = sklearn.metrics.auc(data['velocity'], data['load_median'])\n",
        "\n",
        "    zero_velocity_load = data['load_median'].max()\n",
        "    zero_velocity_load_lower = data['load_lower'].max()\n",
        "    zero_velocity_load_upper = data['load_upper'].max()\n",
        "\n",
        "    zero_load_velocity = data['velocity'].max()\n",
        "\n",
        "    exercise = data['exercise'].values[0]\n",
        "    workout_time = pd.to_datetime(data['workout_start_time'].values[0]).strftime('%Y-%m-%d')\n",
        "\n",
        "    ax.plot('velocity',\n",
        "            'load',\n",
        "            marker = 'o',\n",
        "            linestyle = '',\n",
        "            label = 'Observed',\n",
        "            zorder = 2,\n",
        "            data = data)\n",
        "    \n",
        "    ax.plot('velocity',\n",
        "            'load_median',\n",
        "            label = 'Median',\n",
        "            zorder = 1,\n",
        "            data = data)\n",
        "\n",
        "    ax.fill_between(x = 'velocity',\n",
        "                    y1 = 'load_lower',\n",
        "                    y2 = 'load_upper',\n",
        "                    alpha = 0.5,\n",
        "                    color = '#ffcd3c',\n",
        "                    label = '90% HDI',\n",
        "                    zorder = 0,\n",
        "                    data = data)\n",
        "\n",
        "    ax.set_title(f'{exercise.title()}\\n{workout_time}',\n",
        "                 fontsize = 'large',\n",
        "                 fontweight = 'bold')\n",
        "    \n",
        "    ax.annotate(f'Zero Velocity Load: {zero_velocity_load:.0f} [{zero_velocity_load_lower:.0f}-{zero_velocity_load_upper:.0f}] kg\\nZero Load Velocity: {zero_load_velocity:.2f} m/s\\nArea Under Curve: {auc:.0f}',\n",
        "                xy = [0.9855, 0.865],\n",
        "                xycoords = 'axes fraction',\n",
        "                horizontalalignment = 'right',\n",
        "                verticalalignment = 'top',\n",
        "                bbox = {'boxstyle': 'round',\n",
        "                        'edgecolor': '#bcbcbc',\n",
        "                        'facecolor': '#eeeeee'})\n",
        "    \n",
        "    ax.legend()\n",
        "    \n",
        "    ax.set_xlabel('Velocity')\n",
        "    ax.set_ylabel('Load')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SM7FWHFkbKqZ"
      },
      "source": [
        "### Plot Last Sessions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTexP3bGYmth"
      },
      "outputs": [],
      "source": [
        "exercise_last_session_ids = regr_data.groupby(['exercise_id'])['session_id'].max().values\n",
        "n_exercises = len(exercise_last_session_ids)\n",
        "\n",
        "n_cols = 1 if n_exercises == 1 else 2\n",
        "n_rows = 1 if n_exercises <= 2 else  np.ceil(n_exercises/2).astype(int)\n",
        "\n",
        "fig, axes = plt.subplots(n_cols, n_rows, figsize = (15, 15))\n",
        "\n",
        "axes = np.array([axes])\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i in np.arange(n_exercises):\n",
        "    plot_session(prediction_df, exercise_last_session_ids[i], axes[i])\n",
        "\n",
        "fig.suptitle('LAST SESSION PER EXERCISE',\n",
        "             fontweight = 'bold',\n",
        "             fontsize = 'x-large')\n",
        "\n",
        "fig.tight_layout()\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "DihdfgnGBWuo",
        "blDhXZLZQ4BB",
        "jXNFV7da7HqZ",
        "VdWQyE4gQ8BF",
        "ubofKRwqoci-",
        "U_PgwhIy7mF0",
        "uBrZ4RuhqIdC",
        "4YK2zkkNcJUS",
        "BnLyjybwbGIc"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}