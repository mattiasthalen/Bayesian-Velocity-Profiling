{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mattiasthalen/Bayesian-Velocity-Profiling/blob/main/Bayesian_Velocity_Profiling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9dMSQcQ5Lb3"
      },
      "source": [
        "# Bayesian Velocity Profiling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DihdfgnGBWuo"
      },
      "source": [
        "## Setup Environment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVaqZX-3OTWl"
      },
      "source": [
        "### Load Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R43yCeSDP5BD",
        "outputId": "174a7e7e-81f4-43e2-9498-bb8fe3e5e980"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import pint_xarray\n",
        "import warnings\n",
        "import os\n",
        "#import jax\n",
        "#import numpyro\n",
        "import pyro\n",
        "import torch\n",
        "\n",
        "import sklearn.preprocessing\n",
        "import sklearn.metrics\n",
        "#import pymc.sampling_jax\n",
        "\n",
        "#import pymc as pm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import arviz as az\n",
        "import xarray as xr\n",
        "\n",
        "from collections import OrderedDict\n",
        "from urllib.parse import unquote\n",
        "\n",
        "from urllib.parse import unquote\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from typing import Callable, Optional, Dict, List, Union, NoReturn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PQgvlBLOjTS"
      },
      "source": [
        "### Set Global Vars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#import warnings\n",
        "#warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#numpyro.util.set_platform('cpu')\n",
        "#n_devices = len(jax.devices())\n",
        "#numpyro.util.set_host_device_count(n_devices)\n",
        "#rng_key = jax.random.PRNGKey(33)\n",
        "#jax.devices()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hdi_prob = 0.9\n",
        "\n",
        "csv_path = 'https://raw.githubusercontent.com/mattiasthalen/Bayesian-Velocity-Profiling/main/RepOne_Data_Export.csv'\n",
        "\n",
        "plt.style.use('bmh')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blDhXZLZQ4BB"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubofKRwqoci-"
      },
      "source": [
        "### ETL Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyqfPoReqF_Y",
        "outputId": "ade2acee-a453-483d-844c-b2af087f87b6"
      },
      "outputs": [],
      "source": [
        "def extract(csv_path, **kwargs):\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    df.columns = (df.columns\n",
        "                    .str.lower()\n",
        "                    .str.replace(' \\(m/s\\)', '', regex = True)\n",
        "                    .str.replace(' \\(mm\\)', '', regex = True)\n",
        "                    .str.replace(' \\(sec\\)', '', regex = True)\n",
        "                    .str.replace(' \\(%\\)', '', regex = True)\n",
        "                    .str.replace(' ', '_', regex = True))\n",
        "        \n",
        "    df.rename(columns = {'weight': 'load'}, inplace = True)\n",
        "\n",
        "    df['workout_start_time'] = pd.to_datetime(df['workout_start_time'], format = '%d/%m/%Y, %H:%M:%S')\n",
        "\n",
        "    df.dropna(subset = ['exercise'], inplace = True)\n",
        "    df['rest_time'] = pd.to_timedelta(df['rest_time'])\n",
        "\n",
        "    # Correct split session\n",
        "    df['set'].mask((df['exercise'] == 'deadlift') & (df['workout_start_time'] == pd.to_datetime('2020-12-30 13:06:04')), df['set'] + 7, inplace = True)\n",
        "    df.replace({'workout_start_time': pd.to_datetime('2020-12-30 13:06:04')}, pd.to_datetime('2020-12-30 12:53:09'), inplace = True)\n",
        "    df.replace({'workout_start_time': pd.to_datetime('2021-01-07 11:50:22')}, pd.to_datetime('2021-01-07 11:20:07'), inplace = True)\n",
        "    df.replace({'workout_start_time': pd.to_datetime('2021-06-10 12:02:22')}, pd.to_datetime('2021-06-10 11:56:31'), inplace = True)\n",
        "    df.replace({'workout_start_time': pd.to_datetime('2021-06-14 12:06:00')}, pd.to_datetime('2021-06-14 11:57:50'), inplace = True)\n",
        "\n",
        "    # Reindex sets & reps to counter bugs in the extract\n",
        "    df['set'] = df.groupby(['exercise', 'workout_start_time'], group_keys = False)['set'].apply(lambda x: (x != x.shift()).cumsum() - 1)\n",
        "    df['rep'] = df.groupby(['exercise', 'workout_start_time', 'set'], group_keys = False).cumcount()\n",
        "\n",
        "    # Convert from , to . as decimal sign\n",
        "    df['load'] = df['load'].str.replace(',', '.').astype('float')\n",
        "\n",
        "    # Drop rows with tag fail\n",
        "    fail_filter = df['tags'].str.contains('fail', na = False)\n",
        "    df = df[~fail_filter]\n",
        "\n",
        "    # Handle the case when a rep is split into two reps\n",
        "    rep_split_filter = df['tags'].str.contains('rep split', na = False)\n",
        "\n",
        "    rep_split_df = df[rep_split_filter].groupby(['exercise', 'workout_start_time', 'set', 'load', 'metric'], group_keys = False)[['range_of_motion', 'duration_of_rep']].sum()\n",
        "    rep_split_df['avg_velocity'] = rep_split_df['range_of_motion']/1000/rep_split_df['duration_of_rep']\n",
        "    rep_split_df['rep'] = 0\n",
        "    rep_split_df.reset_index(inplace = True)\n",
        "\n",
        "    rep_split_df = rep_split_df.groupby(['exercise', 'workout_start_time', 'set', 'rep'], group_keys = False).max()\n",
        "\n",
        "    df = pd.concat([df[~rep_split_filter], rep_split_df])\n",
        "\n",
        "    # Group to get multi index\n",
        "    df = df.groupby(['exercise', 'workout_start_time', 'set', 'rep'], group_keys = False).max()\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdOFZqLRtlZp",
        "outputId": "676d4acc-b73b-4170-c416-e9a9d1719e96"
      },
      "outputs": [],
      "source": [
        "def transform(df, **kwargs):\n",
        "    # Convert to xarray\n",
        "    ds = df.to_xarray()\n",
        "\n",
        "    # Change Set and Rep to integers\n",
        "    ds['set'] = ds['set'].astype(int)\n",
        "    ds['rep'] = ds['rep'].astype(int)\n",
        "\n",
        "    # Move variables to coords\n",
        "    ds = ds.set_coords(['metric', 'tags'])\n",
        "\n",
        "    # Define UOMs\n",
        "    ds = ds.pint.quantify({'load': 'kg',\n",
        "                            'avg_velocity': 'meter / second',\n",
        "                            'peak_velocity': 'meter / seconds',\n",
        "                            'range_of_motion': 'mm',\n",
        "                            'duration_of_rep': 's'})\n",
        "\n",
        "    # Session meta data\n",
        "    session_stack = ['exercise', 'workout_start_time']\n",
        "    ds['session_max_load'] = ds['load'].stack(stack = session_stack)\\\n",
        "                                        .groupby('stack')\\\n",
        "                                        .reduce(all_nan_max, ...)\\\n",
        "                                        .unstack()\n",
        "\n",
        "    # Set meta data\n",
        "    set_stack = [*session_stack, 'set']\n",
        "    ds['load'] = (ds['load'].stack(stack = set_stack)\n",
        "                            .groupby('stack')\n",
        "                            .reduce(all_nan_max, ...)\n",
        "                            .unstack())\n",
        "\n",
        "    ds['reps'] = (ds['avg_velocity'].stack(stack = set_stack)\n",
        "                                    .groupby('stack')\n",
        "                                    .count(...)\n",
        "                                    .unstack()\n",
        "                                    .where(ds['load'] > 0, drop = True))\n",
        "\n",
        "    ds['set_velocities'] = summarize(ds['avg_velocity'].pint.dequantify())\n",
        "    ds['set_velocities'] = ds['set_velocities'].pint.quantify({ds['set_velocities'].name: 'mps'})\n",
        "\n",
        "    ds.coords['set_type'] = assign_set_type(ds['load'])\n",
        "\n",
        "    # Add the running min top set velocity per exercise\n",
        "    ds['minimum_velocity_threshold'] = (ds['set_velocities'].sel({'aggregation': 'first'})\n",
        "                                                            .where(ds.coords['set_type'] == 'Top Set')\n",
        "                                                            .pint.dequantify()\n",
        "                                                            .stack(stack = session_stack)\n",
        "                                                            .groupby('stack')\n",
        "                                                            .reduce(all_nan_min, ...)\n",
        "                                                            .unstack()\n",
        "                                                            .rolling({'workout_start_time': len(ds['workout_start_time'])},\n",
        "                                                                    min_periods = 1)\n",
        "                                                            .min())\n",
        "    ds['minimum_velocity_threshold'] = ds['minimum_velocity_threshold'].pint.quantify({ds['minimum_velocity_threshold'].name: 'meter / second'})\n",
        "\n",
        "    # Add running max load per exercise\n",
        "    ds['rolling_max_load'] = (ds['load'].pint.dequantify()\n",
        "                                        .stack(stack = session_stack)\n",
        "                                        .groupby('stack')\n",
        "                                        .reduce(all_nan_max, ...)\n",
        "                                        .unstack()\n",
        "                                        .rolling({'workout_start_time': len(ds['workout_start_time'])},\n",
        "                                                min_periods = 1)\n",
        "                                        .max())\n",
        "    ds['rolling_max_load'] = ds['rolling_max_load'].pint.quantify({ds['rolling_max_load'].name: 'kg'})\n",
        "    \n",
        "    # Generate the observation weights\n",
        "    ds['observation_weight'] = 0.5**((1 - ds['minimum_velocity_threshold']/ds['set_velocities'])/0.2)\n",
        "\n",
        "    # Additional session meta data\n",
        "    ds['workup_sets'] = ds['load'].where(ds.coords['set_type'] == 'Work Up', drop = True)\\\n",
        "                                    .stack(stack = session_stack)\\\n",
        "                                    .groupby('stack')\\\n",
        "                                    .count(...)\\\n",
        "                                    .unstack()\n",
        "\n",
        "    #ds['session_regression_coefficients'] = linear_fit(ds, 'load', 'set_velocities', 'set')\n",
        "\n",
        "    #ds['estimated_1rm'] = linear_predict(ds['minimum_velocity_threshold'].pint.dequantify(), ds['session_regression_coefficients'])\n",
        "    #ds['estimated_1rm'] = ds['estimated_1rm'].pint.quantify({ds['estimated_1rm'].name: 'kg'})\n",
        "\n",
        "    #ds['zero_velocity_load'] = linear_predict(0, ds['session_regression_coefficients'])\n",
        "    #ds['zero_velocity_load'] = ds['zero_velocity_load'].pint.quantify({ds['zero_velocity_load'].name: 'kg'})\n",
        "\n",
        "    #ds['zero_load_velocity'] = linear_predict(0, ds['session_regression_coefficients'], reverse = True)\n",
        "    #ds['zero_load_velocity'] = ds['zero_load_velocity'].pint.quantify({ds['zero_load_velocity'].name: 'mps'})\n",
        "\n",
        "    #ds['curve_score'] = ds['zero_velocity_load'].pint.dequantify()*ds['zero_load_velocity'].pint.dequantify()/2\n",
        "\n",
        "    ds['session_volume'] = (ds['load'] * ds['reps']).stack(stack = session_stack).groupby('stack').sum(...).unstack()\n",
        "    #ds['session_relative_volume'] = ds['session_volume']/ds['estimated_1rm']\n",
        "\n",
        "    # Rep meta data\n",
        "    #ds['rep_exertion'] = linear_predict(ds['avg_velocity'].pint.dequantify(), ds['session_regression_coefficients'])/ds['estimated_1rm'].pint.dequantify()\n",
        "    #ds['rep_force'] = (ds['load']*ds['range_of_motion'].pint.to('meter')/ds['duration_of_rep']**2).pint.to('N')\n",
        "    #ds['rep_energy'] = (ds['rep_force']*ds['range_of_motion'].pint.to('meter')).pint.to('J')\n",
        "\n",
        "    # Session meta data\n",
        "    #ds['session_exertion_load'] = ds['rep_exertion'].stack(stack = ['exercise', 'workout_start_time']).groupby('stack').reduce(all_nan_sum, ...).unstack().pint.dequantify()\n",
        "\n",
        "    # Add PR coordinates\n",
        "    ds.coords['max_load_pr_flag'] = ds['rolling_max_load'].diff('workout_start_time') > 0\n",
        "    ds.coords['max_load_pr_flag'] = ds.coords['max_load_pr_flag'].fillna(0).astype(int)\n",
        "\n",
        "    # Add indexing for inference\n",
        "    session_shape = [ds.dims[i] for i in session_stack]\n",
        "    ds.coords['session'] = (session_stack, np.arange(np.prod(session_shape)).reshape(session_shape))\n",
        "\n",
        "    observation_shape = [ds.dims[i] for i in set_stack]\n",
        "    ds.coords['observation'] = (set_stack, np.arange(np.prod(observation_shape)).reshape(observation_shape))\n",
        "\n",
        "    return ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8NC-0aKcIVc",
        "outputId": "2bd344e4-19d1-411e-8e5f-c0732db57ebe"
      },
      "outputs": [],
      "source": [
        "def load(csv_path, **kwargs):\n",
        "    df = extract(csv_path, **kwargs)\n",
        "    ds = transform(df, **kwargs)\n",
        "\n",
        "    return ds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fle_jrH0eraj"
      },
      "source": [
        "### Model Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Numpyro"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_numpyro_model(velocity,\n",
        "                        session_idx,\n",
        "                        session_exercise_idx,\n",
        "                        load = None,\n",
        "                        weight = None,\n",
        "                        coords = None,\n",
        "                        **kwargs):\n",
        "    \n",
        "    n_observations = len(velocity)\n",
        "    n_exercises = len(np.unique(session_exercise_idx))\n",
        "    n_sessions = len(np.unique(session_idx))\n",
        "\n",
        "    if weight is None:\n",
        "        weight = np.ones_like(velocity)\n",
        "    \n",
        "    # Global Parameters\n",
        "    intercept_global = numpyro.sample(name = 'intercept_global',\n",
        "                                      fn = numpyro.distributions.Normal(loc = 0.0, scale = 1.0))\n",
        "\n",
        "    intercept_scale_global = numpyro.sample(name = 'intercept_scale_global',\n",
        "                                            fn = numpyro.distributions.HalfNormal(scale = 1.0))\n",
        "\n",
        "\n",
        "    slope_global = numpyro.sample(name = 'slope_global',\n",
        "                                  fn = numpyro.distributions.HalfNormal(scale = 1.0))\n",
        "    \n",
        "    curve_global = numpyro.sample(name = 'curve_global',\n",
        "                                  fn = numpyro.distributions.HalfNormal(scale = 1.0))\n",
        "    \n",
        "    scale_global = numpyro.sample(name = 'scale_global',\n",
        "                                  fn = numpyro.distributions.HalfNormal(scale = 1.0))\n",
        "    \n",
        "    # Exercise Parameters\n",
        "    with numpyro.plate(name = 'exercise', size = n_exercises):\n",
        "        \n",
        "        intercept_exercise = numpyro.sample(name = 'intercept_exercise',\n",
        "                                            fn = numpyro.distributions.Normal(loc = intercept_global, scale = intercept_scale_global))\n",
        "        \n",
        "        intercept_scale_exercise = numpyro.sample(name = 'intercept_scale_exercise',\n",
        "                                                  fn = numpyro.distributions.HalfNormal(scale = 1.0))\n",
        "\n",
        "        slope_exercise = numpyro.sample(name = 'slope_exercise',\n",
        "                                        fn = numpyro.distributions.HalfNormal(scale = slope_global))\n",
        "        \n",
        "        curve_exercise = numpyro.sample(name = 'curve_exercise',\n",
        "                                        fn = numpyro.distributions.HalfNormal(scale = curve_global))\n",
        "\n",
        "    # Session Parameters\n",
        "    with numpyro.plate(name = 'session', size = n_sessions):\n",
        "\n",
        "        intercept_session = numpyro.sample(name = 'intercept_session',\n",
        "                                           fn = numpyro.distributions.Normal(loc = intercept_exercise[session_exercise_idx],\n",
        "                                                                             scale = intercept_scale_exercise[session_exercise_idx]))\n",
        "\n",
        "        slope_session = numpyro.sample(name = 'slope_session',\n",
        "                                       fn = numpyro.distributions.HalfNormal(scale = slope_exercise[session_exercise_idx]))\n",
        "\n",
        "        curve_session = numpyro.sample(name = 'curve_session',\n",
        "                                       fn = numpyro.distributions.HalfNormal(scale = curve_exercise[session_exercise_idx]))\n",
        "\n",
        "    with numpyro.plate(name = 'observation', size = n_observations):\n",
        "        \n",
        "        # Final Parameters\n",
        "        intercept = intercept_session[session_idx]\n",
        "        slope = slope_session[session_idx]\n",
        "        curve = curve_session[session_idx]\n",
        "\n",
        "        # Model error\n",
        "        load_scale = numpyro.deterministic(name = 'load_scale', \n",
        "                                           value = scale_global/weight)\n",
        "\n",
        "        # Estimated Value\n",
        "        load_loc = numpyro.deterministic(name = 'load_loc',\n",
        "                                         value = intercept - slope*velocity - curve*velocity**2)\n",
        "\n",
        "        # Likelihood\n",
        "        load_likelihood = numpyro.sample(name = 'load',\n",
        "                                         fn = numpyro.distributions.Normal(loc = load_loc, scale = load_scale),\n",
        "                                         obs = load)\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Pyro"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_pyro_model(velocity,\n",
        "                     session_idx,\n",
        "                     session_exercise_idx,\n",
        "                     load = None,\n",
        "                     weight = None,\n",
        "                     coords = None,\n",
        "                     **kwargs):\n",
        "    \n",
        "    n_observations = len(velocity)\n",
        "    n_exercises = len(np.unique(session_exercise_idx))\n",
        "    n_sessions = len(np.unique(session_idx))\n",
        "    \n",
        "    velocity = torch.as_tensor(velocity)\n",
        "    load = torch.as_tensor(load)\n",
        "\n",
        "    if weight is None:\n",
        "        weight = np.ones_like(velocity)\n",
        "    \n",
        "    weight = torch.as_tensor(weight)\n",
        "\n",
        "    # Global Parameters\n",
        "    intercept_global = pyro.sample(name = 'intercept_global',\n",
        "                                   fn = pyro.distributions.Normal(loc = 0.0,\n",
        "                                                                  scale = 1.0))\n",
        "    \n",
        "    intercept_scale_global = pyro.sample(name = 'intercept_scale_global',\n",
        "                                         fn = pyro.distributions.HalfNormal(scale = 1.0))\n",
        "    \n",
        "    slope_global = pyro.sample(name = 'slope_global',\n",
        "                               fn = pyro.distributions.HalfNormal(scale = 1.0))\n",
        "    \n",
        "    curve_global = pyro.sample(name = 'curve_global',\n",
        "                               fn = pyro.distributions.HalfNormal(scale = 1.0))\n",
        "    \n",
        "    scale_global = pyro.sample(name = 'scale_global',\n",
        "                               fn = pyro.distributions.HalfNormal(scale = 1.0))\n",
        "    \n",
        "     # Exercise Parameters\n",
        "    with pyro.plate(name = 'exercise', size = n_exercises):\n",
        "        \n",
        "        intercept_exercise = pyro.sample(name = 'intercept_exercise',\n",
        "                                         fn = pyro.distributions.Normal(loc = intercept_global,\n",
        "                                                                        scale = intercept_scale_global))\n",
        "        \n",
        "        intercept_scale_exercise = pyro.sample(name = 'intercept_scale_exercise',\n",
        "                                               fn = pyro.distributions.HalfNormal(scale = 1.0))\n",
        "\n",
        "        slope_exercise = pyro.sample(name = 'slope_exercise',\n",
        "                                     fn = pyro.distributions.HalfNormal(scale = slope_global))\n",
        "        \n",
        "        curve_exercise = pyro.sample(name = 'curve_exercise',\n",
        "                                     fn = pyro.distributions.HalfNormal(scale = curve_global))\n",
        "\n",
        "    # Session Parameters\n",
        "    with pyro.plate(name = 'session', size = n_sessions):\n",
        "\n",
        "        intercept_session = pyro.sample(name = 'intercept_session',\n",
        "                                        fn = pyro.distributions.Normal(loc = intercept_exercise[session_exercise_idx],\n",
        "                                                                       scale = intercept_scale_exercise[session_exercise_idx]))\n",
        "\n",
        "        slope_session = pyro.sample(name = 'slope_session',\n",
        "                                       fn = pyro.distributions.HalfNormal(scale = slope_exercise[session_exercise_idx]))\n",
        "\n",
        "        curve_session = pyro.sample(name = 'curve_session',\n",
        "                                    fn = pyro.distributions.HalfNormal(scale = curve_exercise[session_exercise_idx]))\n",
        "\n",
        "    with pyro.plate(name = 'observation', size = n_observations):\n",
        "        \n",
        "        # Final Parameters\n",
        "        intercept = intercept_session[session_idx]\n",
        "        slope = slope_session[session_idx]\n",
        "        curve = curve_session[session_idx]\n",
        "\n",
        "        # Model error\n",
        "        load_scale = pyro.deterministic(name = 'load_scale', \n",
        "                                        value = scale_global/weight)\n",
        "\n",
        "        # Estimated Value\n",
        "        load_loc = pyro.deterministic(name = 'load_loc',\n",
        "                                      value = intercept - slope*velocity - curve*velocity**2)\n",
        "\n",
        "        # Likelihood\n",
        "        load_likelihood = pyro.sample(name = 'load',\n",
        "                                      fn = pyro.distributions.Normal(loc = load_loc, scale = load_scale),\n",
        "                                      obs = load)\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### PyMC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_pymc_model(velocity,\n",
        "                     load,\n",
        "                     session_idx,\n",
        "                     session_exercise_idx,\n",
        "                     coords,\n",
        "                     weight = None,\n",
        "                     **kwargs):\n",
        "\n",
        "    if weight is None:\n",
        "        weight = np.ones_like(velocity)\n",
        "\n",
        "    with pm.Model() as model:\n",
        "        \n",
        "        # Add coordinates\n",
        "        model.add_coord(name = 'observation',\n",
        "                        values = coords['observation'],\n",
        "                        mutable = True)\n",
        "        \n",
        "        model.add_coord(name = 'exercise',\n",
        "                        values = coords['exercise'],\n",
        "                        mutable = False)\n",
        "        \n",
        "        model.add_coord(name = 'session',\n",
        "                        values = coords['session'],\n",
        "                        mutable = False)\n",
        "\n",
        "        # Add inputs\n",
        "        velocity = pm.MutableData(name = 'velocity',\n",
        "                                  value = velocity,\n",
        "                                  dims = 'observation')\n",
        "\n",
        "        weight = pm.MutableData(name = 'weight',\n",
        "                                value = weight,\n",
        "                                dims = 'observation')\n",
        "\n",
        "        session_idx = pm.MutableData(name = 'session_idx',\n",
        "                                     value = session_idx,\n",
        "                                     dims = 'observation')\n",
        "\n",
        "        # Global Parameters\n",
        "        intercept_global = pm.Normal(name = 'intercept_global',\n",
        "                                        mu = 0.0,\n",
        "                                        sigma = 1.0)\n",
        "        \n",
        "        intercept_sigma_global = pm.HalfNormal(name = 'intercept_sigma_global',\n",
        "                                                sigma = 1.0)\n",
        "        \n",
        "        slope_global = pm.HalfNormal(name = 'slope_global',\n",
        "                                        sigma = 1.0)\n",
        "        \n",
        "        curve_global = pm.HalfNormal(name = 'curve_global',\n",
        "                                        sigma = 1.0)\n",
        "        \n",
        "        error_global = pm.HalfNormal(name = 'error_global',\n",
        "                                        sigma = 1.0)\n",
        "        \n",
        "        # Exercise Parameters\n",
        "        intercept_offset_exercise = pm.Normal(name = 'intercept_offset_exercise',\n",
        "                                                mu = 0.0,\n",
        "                                                sigma = 1.0,\n",
        "                                                dims = 'exercise')\n",
        "        \n",
        "        intercept_exercise = pm.Deterministic(name = 'intercept_exercise',\n",
        "                                                var = intercept_global + intercept_sigma_global*intercept_offset_exercise,\n",
        "                                                dims = 'exercise')\n",
        "        \n",
        "        intercept_sigma_exercise = pm.HalfNormal(name = 'intercept_sigma_exercise',\n",
        "                                                    sigma = 1.0,\n",
        "                                                    dims = 'exercise')\n",
        "        \n",
        "        slope_exercise = pm.HalfNormal(name = 'slope_exercise',\n",
        "                                        sigma = slope_global,\n",
        "                                        dims = 'exercise')\n",
        "        \n",
        "        curve_exercise = pm.HalfNormal(name = 'curve_exercise',\n",
        "                                        sigma = curve_global,\n",
        "                                        dims = 'exercise')\n",
        "\n",
        "        # Session Parameters\n",
        "        intercept_offset_session = pm.Normal(name = 'intercept_offset_session',\n",
        "                                                mu = 0.0,\n",
        "                                                sigma = 1.0,\n",
        "                                                dims = 'session')\n",
        "\n",
        "        intercept_session = pm.Deterministic(name = 'intercept_session',\n",
        "                                                var = (intercept_exercise[session_exercise_idx]\n",
        "                                                        + intercept_sigma_exercise[session_exercise_idx]\n",
        "                                                        * intercept_offset_session),\n",
        "                                                dims = 'session')\n",
        "        \n",
        "        slope_session = pm.HalfNormal(name = 'slope_session',\n",
        "                                        sigma = slope_exercise[session_exercise_idx],\n",
        "                                        dims = 'session')\n",
        "\n",
        "        curve_session = pm.HalfNormal(name = 'curve_session',\n",
        "                                        sigma = curve_exercise[session_exercise_idx],\n",
        "                                        dims = 'session')\n",
        "        \n",
        "        # Final Parameters\n",
        "        intercept = intercept_session[session_idx]\n",
        "        slope = slope_session[session_idx]\n",
        "        curve = curve_session[session_idx]\n",
        "        error = pm.Deterministic(name = 'error_observation', \n",
        "                                 var = error_global/weight,\n",
        "                                 dims = 'observation')\n",
        "\n",
        "        # Estimated Value\n",
        "        load_mu = pm.Deterministic(name = 'mu',\n",
        "                                    var = intercept - slope*velocity - curve*velocity**2,\n",
        "                                    dims = 'observation')\n",
        "\n",
        "        # Likelihood\n",
        "        load_likelihood = pm.Normal(name = 'load',\n",
        "                                    mu = load_mu,\n",
        "                                    sigma = error,\n",
        "                                    observed = load,\n",
        "                                    dims = 'observation')\n",
        "        \n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXAaBR2b7d_Y",
        "outputId": "70036981-39c6-4c0e-e98a-bae1257b9d45"
      },
      "outputs": [],
      "source": [
        "def build_model(model_backend = 'pymc',\n",
        "                **kwargs):\n",
        "\n",
        "    if model_backend == 'pymc':\n",
        "        model = build_pymc_model(**kwargs)\n",
        "        display(pm.model_to_graphviz(model))\n",
        "    \n",
        "    if model_backend == 'pyro':\n",
        "        model = pyro.handlers.reparam(fn = build_pyro_model,\n",
        "                                      config = {'intercept_exercise': pyro.infer.reparam.LocScaleReparam(0.0),\n",
        "                                                   'intercept_session': pyro.infer.reparam.LocScaleReparam(0.0)})\n",
        "        display(pyro.render_model(model, model_kwargs = kwargs))\n",
        "    \n",
        "    if model_backend == 'numpyro':\n",
        "        model = numpyro.handlers.reparam(fn = build_numpyro_model,\n",
        "                                         config = {'intercept_exercise': numpyro.infer.reparam.LocScaleReparam(0.0),\n",
        "                                                   'intercept_session': numpyro.infer.reparam.LocScaleReparam(0.0)})\n",
        "                                                   \n",
        "        display(numpyro.render_model(model, model_kwargs = kwargs))\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hsrh2fMJgdWR",
        "outputId": "a094e857-882e-4da7-98ee-8fcbd2093362"
      },
      "outputs": [],
      "source": [
        "def sample(model,\n",
        "           prior_predictive = True,\n",
        "           posterior_predictive = True,\n",
        "           sampler = 'pymc',\n",
        "           sampler_kwargs = None,\n",
        "           prior_kwargs = None,\n",
        "           post_pred_kwargs = None,\n",
        "           **kwargs):\n",
        "\n",
        "    if sampler_kwargs is None:\n",
        "        sampler_kwargs = {}\n",
        "\n",
        "    if prior_kwargs is None:\n",
        "        prior_kwargs = {}\n",
        "\n",
        "    if post_pred_kwargs is None:\n",
        "        post_pred_kwargs = {}\n",
        "\n",
        "    if sampler == 'pymc':\n",
        "        sampler_fn = pm.sample\n",
        "    \n",
        "    if sampler == 'numpyro':\n",
        "        sampler_fn = pm.sampling_jax.sample_numpyro_nuts\n",
        "    \n",
        "    if sampler == 'blackjax':\n",
        "        sampler_fn = pm.sampling_jax.sample_blackjax_nuts\n",
        "\n",
        "    with model:\n",
        "        inference_data = sampler_fn(**sampler_kwargs)\n",
        "        \n",
        "        if prior_predictive:\n",
        "            inference_data.extend(pm.sample_prior_predictive(**prior_kwargs))\n",
        "\n",
        "        if posterior_predictive:\n",
        "            inference_data.extend(pm.sample_posterior_predictive(trace = inference_data, **post_pred_kwargs))\n",
        "\n",
        "    return inference_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdWQyE4gQ8BF"
      },
      "source": [
        "### Plotting Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 853
        },
        "id": "NyXo9kAo_TFq",
        "outputId": "4a3ac4f6-741f-4c5f-8e6b-4d5d14ff1db9"
      },
      "outputs": [],
      "source": [
        "def plot_last_session(inference_data, df, exercises):\n",
        "    n_exercises = len(exercises)\n",
        "\n",
        "    n_rows = np.ceil(n_exercises / 2).astype(int)\n",
        "    n_cols = n_exercises - n_rows\n",
        "\n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize = (15, 10), sharex = True, sharey = True)\n",
        "\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for i, ax in enumerate(axes):\n",
        "\n",
        "        exercise = exercises[i]\n",
        "\n",
        "        last_session = df[df['exercise'] == exercise]['session'].max()\n",
        "        last_session_observations = df[df['session'] == last_session]['observation'].values\n",
        "\n",
        "        last_session_idata = inference_data.sel(observation = last_session_observations, session = last_session, exercise = exercise)\n",
        "\n",
        "        az.plot_lm(idata = last_session_idata,\n",
        "                x = 'velocity',\n",
        "                y = 'load',\n",
        "                y_hat = 'load',\n",
        "                y_model = 'mu',\n",
        "                kind_pp = 'hdi',\n",
        "                kind_model = 'hdi',\n",
        "                y_kwargs = {'marker': '.',\n",
        "                            'markersize': 9,\n",
        "                            'color': '#0571b0',\n",
        "                            'label': 'Observed Data',\n",
        "                            'zorder': 3},\n",
        "                y_hat_fill_kwargs = {'fill_kwargs': {'zorder': 0,\n",
        "                                                        'alpha': 0.6},\n",
        "                                        'color': '#92c5de',\n",
        "                                        'hdi_prob': hdi_prob},\n",
        "                y_model_mean_kwargs = {'zorder': 2,\n",
        "                                        'color': '#ca0020',\n",
        "                                        'linewidth': 1,\n",
        "                                        'linestyle': 'dashed'},\n",
        "                y_model_fill_kwargs = {'zorder': 1,\n",
        "                                        'color': '#f4a582',\n",
        "                                        'alpha': 0.6,\n",
        "                                        #'fill_kwargs': {'zorder': 1},\n",
        "                                        #'hdi_kwargs': {'hdi_prob': hdi_prob}\n",
        "                                        },\n",
        "                legend = False,\n",
        "                axes = ax)\n",
        "        \n",
        "        ax.set_title(exercise.title(), fontweight = 'bold')\n",
        "        ax.set_xlabel('Velocity', fontweight = 'bold')\n",
        "        ax.set_ylabel('Load', fontweight = 'bold')\n",
        "        ax.legend(loc = 'upper right')\n",
        "\n",
        "    fig.suptitle('LAST SESSION PER EXERCISE',\n",
        "                fontweight = 'bold',\n",
        "                fontsize = 'x-large')\n",
        "\n",
        "    plt.draw()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMHAdbMtRBdk",
        "outputId": "a094bf6d-89db-4def-f0f3-3670bdca3eb4"
      },
      "outputs": [],
      "source": [
        "def plot_pbc(ds, exercise, data_var, window = 20, signal_window = 8, ax = None, **kwargs):\n",
        "    df = ds[data_var].sel({'exercise': exercise}, drop = True)\\\n",
        "                     .drop_vars(['training_cycle', 'cycle_type', 'max_weight_pr_flag'])\\\n",
        "                     .to_dataframe()\\\n",
        "                     .dropna()\n",
        "    \n",
        "    df['moving_average'] = df[data_var].sort_index(ascending = False)\\\n",
        "                                       .rolling(window, min_periods = 1)\\\n",
        "                                       .mean()\n",
        "    \n",
        "    df['moving_range'] = df[data_var].diff(-1)\\\n",
        "                                     .abs()\\\n",
        "                                     .sort_index(ascending = False)\\\n",
        "                                     .rolling(window, min_periods = 1)\\\n",
        "                                     .mean()\n",
        "\n",
        "    df['process_average'] = df['moving_average']\n",
        "    df['process_range'] = df['moving_range']\n",
        "    df['signal'] = None\n",
        "    df['signal_min'] = None\n",
        "    df['signal_max'] = None\n",
        "    df['signal_above_average'] = None\n",
        "    df['signal_below_average'] = None\n",
        "\n",
        "    n_rows = len(df)\n",
        "    previous_signal_id = 0\n",
        "\n",
        "    for row in np.arange(n_rows):\n",
        "        first_row = row == 0\n",
        "        sufficient_rows_left = n_rows - row >= window\n",
        "\n",
        "        signal_start_id = np.max([8, row - signal_window])\n",
        "\n",
        "        df['signal_min'][row] = df[data_var][signal_start_id:row].min()\n",
        "        df['signal_max'][row] = df[data_var][signal_start_id:row].max()\n",
        "\n",
        "        df['signal_above_average'][row] = (df['signal_min'][row] > df['process_average'][row - 1])\n",
        "        df['signal_below_average'][row] = (df['signal_max'][row] < df['process_average'][row - 1])\n",
        "\n",
        "        signal_open = (first_row) | (row >= previous_signal_id + window)\n",
        "        signal = (signal_open) & (sufficient_rows_left) & (first_row | df['signal_above_average'][row] | df['signal_below_average'][row])\n",
        "        df['signal'][row] = signal\n",
        "        \n",
        "        df['process_average'][row] =  df['process_average'][row - 1]\n",
        "        df['process_range'][row] =  df['process_range'][row - 1]\n",
        "\n",
        "        if signal:\n",
        "            previous_signal_id = row\n",
        "            df['process_average'][row] =  df['moving_average'][row]\n",
        "            df['process_range'][row] =  df['moving_range'][row]\n",
        "        else:\n",
        "            df['process_average'][row] =  df['process_average'][row - 1]\n",
        "            df['process_range'][row] =  df['process_range'][row - 1]\n",
        "\n",
        "    df['lower_limit_1'] = df['process_average'] - df['process_range']/1.128\n",
        "    df['upper_limit_1'] = df['process_average'] + df['process_range']/1.128\n",
        "    df['lower_limit_2'] = df['process_average'] - df['process_range']*2/1.128\n",
        "    df['upper_limit_2'] = df['process_average'] + df['process_range']*2/1.128\n",
        "    df['lower_limit_3'] = df['process_average'] - df['process_range']*3/1.128\n",
        "    df['upper_limit_3'] = df['process_average'] + df['process_range']*3/1.128\n",
        "\n",
        "    if ax is None:\n",
        "        ax = plt.gca()\n",
        "\n",
        "    ax.scatter(df.index, df[data_var], marker = '.', alpha = 0.6)\n",
        "    ax.plot(df.index, df['process_average'])\n",
        "    ax.plot(df.index, df['lower_limit_3'])\n",
        "    ax.plot(df.index, df['upper_limit_3'])\n",
        "\n",
        "    ax.fill_between(df.index,df['lower_limit_1'], df['upper_limit_1'], alpha = 0.3)\n",
        "\n",
        "    ax.set_xlabel('')\n",
        "    ax.set_ylabel('')\n",
        "    #ax.set_ylim([0, None])\n",
        "    ax.tick_params(labelrotation = 90)\n",
        "    ax.grid()\n",
        "\n",
        "    return ax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rTUijXu0i27",
        "outputId": "f54867f5-2ee3-43ee-db6a-282e8e206184"
      },
      "outputs": [],
      "source": [
        "def plot_kpis(ds, exercise, vars, **kwargs):\n",
        "    var_titles = [var.title().replace('_', ' ').replace('1Rm', '1RM') for var in vars]\n",
        "\n",
        "    n_vars = len(vars)\n",
        "\n",
        "    n_cols = 1\n",
        "\n",
        "    if n_vars > 1:\n",
        "        n_cols = 2\n",
        "    \n",
        "    n_rows = 1\n",
        "\n",
        "    if n_vars > 2:\n",
        "        n_rows = np.ceil(n_vars/n_cols).astype(int)\n",
        "    \n",
        "    figsize = np.array([6, 3]) * [n_cols, n_rows]\n",
        "\n",
        "    fig, axes = plt.subplots(ncols = n_cols,\n",
        "                            nrows = n_rows,\n",
        "                            constrained_layout = True,\n",
        "                            figsize = figsize,\n",
        "                            sharex = True)\n",
        "    \n",
        "    axes = [ax for row in axes for ax in row]\n",
        "\n",
        "    for key, val in enumerate(vars):\n",
        "        plot_pbc(ds, exercise, vars[key], ax = axes[key])\n",
        "        title = var_titles[key]\n",
        "        axes[key].set_title(title)\n",
        "\n",
        "    fig.suptitle(f'{exercise.title()} KPIs', fontsize = 16)\n",
        "    fig.supxlabel('Workout Start Time')\n",
        "\n",
        "    plt.draw()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_PgwhIy7mF0"
      },
      "source": [
        "### Miscellaneous Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnyih5CgK4EY",
        "outputId": "44150b5f-a94c-481a-d21b-b91fb4310be5"
      },
      "outputs": [],
      "source": [
        "def assign_set_type(da, **kwargs):\n",
        "    set_category = xr.where(da['set'] < da.idxmax('set'), 'Work Up', np.nan)\n",
        "    set_category = xr.where(da['set'] == da.idxmax('set'), 'Top Set', set_category)\n",
        "    set_category = xr.where(da['set'] > da.idxmax('set'), 'Back Off', set_category)\n",
        "    set_category = xr.where(np.isnan(da), np.nan, set_category)\n",
        "    \n",
        "    return set_category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FV50KZquxdj5",
        "outputId": "076dc0ae-42fb-430c-deff-e092a7c9fcc5"
      },
      "outputs": [],
      "source": [
        "def agg_summarize(x, **kwargs):\n",
        "    x = x[np.isfinite(x)]\n",
        "\n",
        "    if len(x) > 0:\n",
        "        min = np.min(x)\n",
        "        max = np.max(x)\n",
        "        first = x[0]\n",
        "        last = x[-1]\n",
        "        peak_end = np.mean([min, last])\n",
        "        mean = np.mean(x)\n",
        "        median = np.median(x)\n",
        "        hdi = az.hdi(x)\n",
        "        result = np.array([min, max, first, last, peak_end, mean, median, *hdi])\n",
        "    else:\n",
        "        result = np.array([np.nan]*9)\n",
        "    \n",
        "    return result\n",
        "\n",
        "def summarize(x, reduce_dim = 'rep', **kwargs):\n",
        "    summaries = xr.apply_ufunc(agg_summarize,\n",
        "                               x,\n",
        "                               vectorize = True,\n",
        "                               input_core_dims = [[reduce_dim]],\n",
        "                               output_core_dims = [['aggregation']])\n",
        "    \n",
        "    summaries['aggregation'] = ['min', 'max', 'first', 'last', 'peak_end', 'mean', 'median', 'hdi_lower', 'hdi_upper']\n",
        "    \n",
        "    return summaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTGyzLdT0s5G",
        "outputId": "a796b788-4cd5-4665-c60a-6533301d0744"
      },
      "outputs": [],
      "source": [
        "def agg_hdi_summary(x, **kwargs):\n",
        "    mean = x.mean()\n",
        "    median = np.median(x)\n",
        "    hdi = az.hdi(x)\n",
        "\n",
        "    return np.array([mean, median, *hdi])\n",
        "\n",
        "def hdi_summary(x, reduce_dim = 'sample', **kwargs):\n",
        "    try:\n",
        "        x = x.pint.dequantify()\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    summaries = xr.apply_ufunc(agg_hdi_summary,\n",
        "                               x,\n",
        "                               vectorize = True,\n",
        "                               input_core_dims = [[reduce_dim]],\n",
        "                               output_core_dims = [['hdi_aggregation']])\n",
        "    \n",
        "    summaries['hdi_aggregation'] = ['mean', 'median', 'hdi_lower', 'hdi_upper']\n",
        "    \n",
        "    return summaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gisKtoreeVSI",
        "outputId": "5d533bd6-a0cc-48f6-8b98-64d6baf5aefa"
      },
      "outputs": [],
      "source": [
        "def all_nan_summary(x, mode = 'mean', **kwargs):\n",
        "    if np.all(np.isnan(x)):\n",
        "        return np.nan\n",
        "    elif mode == 'max':\n",
        "        return np.nanmax(x)\n",
        "    elif mode == 'min':\n",
        "        return np.nanmin(x)\n",
        "    elif mode == 'mean':\n",
        "        return np.nanmean(x)\n",
        "    elif mode == 'sum':\n",
        "        return np.nansum(x)\n",
        "        \n",
        "def all_nan_max(x, **kwargs):\n",
        "    return all_nan_summary(x, 'max', **kwargs)\n",
        "        \n",
        "def all_nan_min(x, **kwargs):\n",
        "    return all_nan_summary(x, 'min', **kwargs)\n",
        "        \n",
        "def all_nan_mean(x, **kwargs):\n",
        "    return all_nan_summary(x, 'mean', **kwargs)\n",
        "        \n",
        "def all_nan_sum(x, **kwargs):\n",
        "    return all_nan_summary(x, 'sum', **kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8G37MnsGlipD"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 548
        },
        "id": "UE4reGBCb-pm",
        "outputId": "db2af12a-d345-498f-bae6-d77466995c12"
      },
      "outputs": [],
      "source": [
        "ds = load('RepOne_Data_Export.csv')\n",
        "\n",
        "ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "id": "pR5KAgvkBCs4",
        "outputId": "5a2e6304-a2d6-48a5-ad00-f02bb62aac2f"
      },
      "outputs": [],
      "source": [
        "regr_data = (ds[['load', 'set_velocities', 'observation_weight', 'observation', 'session']]\n",
        "             .pint.dequantify()\n",
        "             .sel({'aggregation': 'max'}, drop = True)\n",
        "             .where(ds.coords['set_type'] != 'Back Off')\n",
        "             .where(ds.coords['exercise'] != 'front squat')\n",
        "             .drop_vars(['set_type', 'max_load_pr_flag'])\n",
        "             .to_dataframe()\n",
        "             .dropna()\n",
        "             .reset_index()\n",
        "             .drop(['set', 'workout_start_time'], axis = 1)\n",
        "             .rename(columns = {'set_velocities': 'velocity'}))\n",
        "\n",
        "# Scale data to simplify inference\n",
        "load_standardizer = sklearn.preprocessing.StandardScaler()\n",
        "regr_data['load_std'] = load_standardizer.fit_transform(regr_data['load'].values.reshape(-1, 1))\n",
        "\n",
        "velocity_standardizer = sklearn.preprocessing.StandardScaler()\n",
        "regr_data['velocity_std'] = velocity_standardizer.fit_transform(regr_data['velocity'].values.reshape(-1, 1))\n",
        "\n",
        "regr_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generate Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 936
        },
        "id": "wfAEig5r7sUw",
        "outputId": "0aa7191a-2906-44b9-e9c0-3b0f5f6ac5ea"
      },
      "outputs": [],
      "source": [
        "velocity_std = regr_data['velocity_std'].values\n",
        "load_std = regr_data['load_std'].values\n",
        "observation_weight = regr_data['observation_weight'].values\n",
        "observation = regr_data['observation'].values\n",
        "\n",
        "exercises = regr_data['exercise'].values\n",
        "exercise_encoder = sklearn.preprocessing.LabelEncoder()\n",
        "exercise_encoder.fit(exercises)\n",
        "\n",
        "sessions = regr_data['session'].values\n",
        "session_encoder = sklearn.preprocessing.LabelEncoder()\n",
        "session_idx = session_encoder.fit_transform(sessions)\n",
        "\n",
        "session_exercise = (regr_data.reset_index()[['session', 'exercise']]\n",
        "                    .drop_duplicates()\n",
        "                    .set_index('session', verify_integrity = True)\n",
        "                    .sort_index()['exercise']\n",
        "                    .values)\n",
        "\n",
        "session_exercise_idx = exercise_encoder.transform(session_exercise)\n",
        "\n",
        "coords = {'observation': observation,\n",
        "          'exercise': exercise_encoder.classes_,\n",
        "          'session': session_encoder.classes_}\n",
        "\n",
        "model_data = {'velocity': velocity_std,\n",
        "              'load': load_std,\n",
        "              'weight': observation_weight,\n",
        "              'session_idx': session_idx,\n",
        "              'session_exercise_idx': session_exercise_idx,\n",
        "              'coords': coords}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Pyro"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pyro_model = build_pyro_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pyro.render_model(model = pyro_model,\n",
        "                  model_kwargs = model_data,\n",
        "                  render_distributions = True,\n",
        "                  render_params = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nuts_kernel = pyro.infer.NUTS(pyro_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mcmc = pyro.infer.MCMC(nuts_kernel,\n",
        "                       num_samples = 500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mcmc.run(**model_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sample_pyro(model,\n",
        "                data,\n",
        "                num_samples = 1000,\n",
        "                num_warmup = 1000,\n",
        "                num_chains = 2,\n",
        "                target_accept_prob = 0.8,\n",
        "                prior_num_samples = 500,\n",
        "                **kwargs):\n",
        "\n",
        "    nuts_kernel = pyro.infer.NUTS(model,\n",
        "                                  target_accept_prob = target_accept_prob)\n",
        "\n",
        "    mcmc = pyro.infer.MCMC(nuts_kernel,\n",
        "                           num_samples = num_samples,\n",
        "                           num_warmup = num_warmup,\n",
        "                           num_chains = num_chains)\n",
        "\n",
        "    mcmc.run(**data)\n",
        "\n",
        "    posterior_samples = mcmc.get_samples()\n",
        "\n",
        "    prior = pyro.infer.Predictive(model, num_samples = prior_num_samples)\n",
        "    prior_samples = prior(**data)\n",
        "\n",
        "    posterior_predictive = pyro.infer.Predictive(model, posterior_samples = posterior_samples)\n",
        "    posterior_predictive_samples = posterior_predictive(data)\n",
        "\n",
        "    inference_data = az.from_numpyro(mcmc,\n",
        "                                     prior = prior_samples,\n",
        "                                     posterior_predictive = posterior_predictive_samples)\n",
        "    \n",
        "    return inference_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Numpyro"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "numpyro_model = build_model(model_backend = 'numpyro',\n",
        "                            render_model = True,\n",
        "                            **model_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sample_numpyro(model,\n",
        "                   data,\n",
        "                   rng_key,\n",
        "                   num_samples = 1000,\n",
        "                   num_warmup = 1000,\n",
        "                   num_chains = 2,\n",
        "                   target_accept_prob = 0.8,\n",
        "                   prior_num_samples = 500,\n",
        "                   **kwargs):\n",
        "\n",
        "    mcmc_key, rng_key = jax.random.split(rng_key)\n",
        "\n",
        "    nuts_kernel = numpyro.infer.NUTS(model,\n",
        "                                     target_accept_prob = target_accept_prob)\n",
        "\n",
        "    mcmc = numpyro.infer.MCMC(nuts_kernel,\n",
        "                              num_samples = num_samples,\n",
        "                              num_warmup = num_warmup,\n",
        "                              num_chains = num_chains)\n",
        "\n",
        "    mcmc.run(mcmc_key, **data)\n",
        "\n",
        "    posterior_samples = mcmc.get_samples()\n",
        "\n",
        "    prior_key, rng_key = jax.random.split(rng_key)\n",
        "    prior = numpyro.infer.Predictive(model, num_samples = prior_num_samples)\n",
        "    prior_samples = prior(prior_key, **data)\n",
        "\n",
        "    posterior_key, rng_key = jax.random.split(rng_key)\n",
        "    posterior_predictive = numpyro.infer.Predictive(model, posterior_samples = posterior_samples)\n",
        "    posterior_predictive_samples = posterior_predictive(posterior_key, **data)\n",
        "\n",
        "    inference_data = az.from_numpyro(mcmc,\n",
        "                                     prior = prior_samples,\n",
        "                                     posterior_predictive = posterior_predictive_samples)\n",
        "    \n",
        "    return inference_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### PyMC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pymc_model = build_model(model_backend = 'pymc',\n",
        "                         render_model = True,\n",
        "                         **model_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Sample Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "numpyro_inference_data = sample_numpyro(model = numpyro_model,\n",
        "                                        data = model_data,\n",
        "                                        rng_key = rng_key,\n",
        "                                        num_chains = 1,\n",
        "                                        target_accept_prob = 0.98)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pymc_inference_data = sample(model = pymc_model,\n",
        "                             sampler = 'pymc',\n",
        "                             sampler_kwargs = {'target_accept': 0.98,\n",
        "                                               'draws': 500,\n",
        "                                               'tune': 500,\n",
        "                                               'chains': 1,#n_devices,\n",
        "                                               'progress_bar': True})\n",
        "                        \n",
        "pymc_inference_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pymc_inference_path = pymc_inference_data.to_netcdf('pymc_inference_data.nc')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPHzlAd85joA"
      },
      "source": [
        "## Plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "_ = az.plot_pair(pymc_inference_data,\n",
        "                 var_names = ['intercept_global', 'slope_global', 'curve_global', 'error_global'],\n",
        "                 divergences = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "_ = az.plot_pair(pymc_inference_data,\n",
        "                 var_names = ['intercept_exercise', 'slope_exercise', 'curve_exercise'],\n",
        "                 divergences = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHn7biVVGfGm"
      },
      "source": [
        "### Trace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 568
        },
        "id": "Bhnj0g0liFuZ",
        "outputId": "3691bd82-a074-4d0d-d6c8-52ed37b4005a"
      },
      "outputs": [],
      "source": [
        "az.summary(pymc_inference_data,\n",
        "           var_names = ['~mu', '~session', '~sigma', '~offset', '~observation', 'intercept', 'slope', 'curve', 'error'],\n",
        "           filter_vars = 'like',\n",
        "           hdi_prob = hdi_prob)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kqXuVGp1fO1s",
        "outputId": "6684a7ea-260f-49dc-e2a5-c57096c9613e"
      },
      "outputs": [],
      "source": [
        "_ = az.plot_trace(pymc_inference_data,\n",
        "                  compact = True,\n",
        "                  combined = True,\n",
        "                  figsize = [15, 20],\n",
        "                  var_names = ['intercept_global', 'slope_global', 'curve_global', 'error_global'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mjC1ot7zHnM7",
        "outputId": "9ee7c397-a1e7-4913-ad5e-7db347f59844"
      },
      "outputs": [],
      "source": [
        "_ = az.plot_trace(pymc_inference_data,\n",
        "                  compact = True,\n",
        "                  combined = True,\n",
        "                  figsize = [15, 15],\n",
        "                  var_names = ['intercept_exercise', 'slope_exercise', 'curve_exercise'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NPFs-CfuHtUF",
        "outputId": "89d13783-ec08-4101-a8ae-9eb603647af9"
      },
      "outputs": [],
      "source": [
        "_ = az.plot_trace(pymc_inference_data,\n",
        "                  compact = True,\n",
        "                  combined = True,\n",
        "                  figsize = [15, 15],\n",
        "                  var_names = ['intercept_session', 'slope_session', 'curve_session'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WS5C0NZMpIiY"
      },
      "source": [
        "### Last Session Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_last_session(inference_data = pymc_inference_data,\n",
        "                  df = regr_data,\n",
        "                  exercises = ['squat', 'overhead press', 'deadlift', 'bench press'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''\n",
        "velocity_min = 0\n",
        "velocity_max = np.ceil(np.nanmax(ds['avg_velocity'])*10)/10\n",
        "velocity_round = 0.01\n",
        "\n",
        "velocity_pred = np.arange(velocity_min, velocity_max + velocity_round, velocity_round).round(2)\n",
        "velocity_pred_scaled = velocity_scaler.transform(velocity_pred.reshape(-1, 1)).flatten()\n",
        "\n",
        "session_idx_pred = np.full_like(velocity_pred, session_idx.max()).astype(int)\n",
        "\n",
        "observation_pred = coords['observation'].max() + np.arange(len(velocity_pred)) + 1\n",
        "\n",
        "with model as model_pred:\n",
        "    pm.set_data(new_data = {'velocity': velocity_pred_scaled,\n",
        "                            'session_idx': session_idx_pred},\n",
        "                coords = {'observation': observation_pred})\n",
        "    \n",
        "    pymc_inference_data.extend(pm.sample_posterior_predictive(pymc_inference_data, predictions = True))\n",
        "\n",
        "pymc_inference_data\n",
        "'''"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "DihdfgnGBWuo",
        "l-vn_F0mmmG2",
        "WwuQ_oVXgYW1",
        "bVaqZX-3OTWl",
        "blDhXZLZQ4BB",
        "jXNFV7da7HqZ",
        "VdWQyE4gQ8BF",
        "ubofKRwqoci-",
        "U_PgwhIy7mF0",
        "dPHzlAd85joA",
        "HEJwabo1cFOl",
        "wrudL7fm6TWx",
        "s5nIjvs78H0W"
      ],
      "include_colab_link": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.6 ('pyro')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "eaa31dd51bd8d16b8d17a9bec0d822a9d0be14d0ba7888386ee0ba79515b2914"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
