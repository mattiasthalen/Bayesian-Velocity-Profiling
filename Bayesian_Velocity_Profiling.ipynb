{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mattiasthalen/Bayesian-Velocity-Profiling/blob/main/Bayesian_Velocity_Profiling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9dMSQcQ5Lb3"
      },
      "source": [
        "# Bayesian Velocity Profiling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DihdfgnGBWuo"
      },
      "source": [
        "## Setup Environment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Python 3.8"
      ],
      "metadata": {
        "id": "B16m_ob_aaON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://repo.anaconda.com/miniconda/Miniconda3-py38_4.12.0-Linux-x86_64.sh\n",
        "!chmod +x Miniconda3-py38_4.12.0-Linux-x86_64.sh\n",
        "!bash ./Miniconda3-py38_4.12.0-Linux-x86_64.sh -b -f -p /usr/local\n",
        "\n",
        "import sys\n",
        "sys.path.append('/usr/local/lib/python3.8/site-packages/')"
      ],
      "metadata": {
        "id": "ZLLEe_I-OOJ-",
        "outputId": "3efe2a9a-277d-4ec7-b30d-42d8d9fb7f83",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-09-22 09:01:46--  https://repo.anaconda.com/miniconda/Miniconda3-py38_4.12.0-Linux-x86_64.sh\n",
            "Resolving repo.anaconda.com (repo.anaconda.com)... 104.16.131.3, 104.16.130.3, 2606:4700::6810:8303, ...\n",
            "Connecting to repo.anaconda.com (repo.anaconda.com)|104.16.131.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 76120962 (73M) [application/x-sh]\n",
            "Saving to: ‘Miniconda3-py38_4.12.0-Linux-x86_64.sh’\n",
            "\n",
            "Miniconda3-py38_4.1 100%[===================>]  72.59M   139MB/s    in 0.5s    \n",
            "\n",
            "2022-09-22 09:01:46 (139 MB/s) - ‘Miniconda3-py38_4.12.0-Linux-x86_64.sh’ saved [76120962/76120962]\n",
            "\n",
            "PREFIX=/usr/local\n",
            "Unpacking payload ...\n",
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "Solving environment: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - _libgcc_mutex==0.1=main\n",
            "    - _openmp_mutex==4.5=1_gnu\n",
            "    - brotlipy==0.7.0=py38h27cfd23_1003\n",
            "    - ca-certificates==2022.3.29=h06a4308_1\n",
            "    - certifi==2021.10.8=py38h06a4308_2\n",
            "    - cffi==1.15.0=py38hd667e15_1\n",
            "    - charset-normalizer==2.0.4=pyhd3eb1b0_0\n",
            "    - colorama==0.4.4=pyhd3eb1b0_0\n",
            "    - conda-content-trust==0.1.1=pyhd3eb1b0_0\n",
            "    - conda-package-handling==1.8.1=py38h7f8727e_0\n",
            "    - conda==4.12.0=py38h06a4308_0\n",
            "    - cryptography==36.0.0=py38h9ce1e76_0\n",
            "    - idna==3.3=pyhd3eb1b0_0\n",
            "    - ld_impl_linux-64==2.35.1=h7274673_9\n",
            "    - libffi==3.3=he6710b0_2\n",
            "    - libgcc-ng==9.3.0=h5101ec6_17\n",
            "    - libgomp==9.3.0=h5101ec6_17\n",
            "    - libstdcxx-ng==9.3.0=hd4cf53a_17\n",
            "    - ncurses==6.3=h7f8727e_2\n",
            "    - openssl==1.1.1n=h7f8727e_0\n",
            "    - pip==21.2.4=py38h06a4308_0\n",
            "    - pycosat==0.6.3=py38h7b6447c_1\n",
            "    - pycparser==2.21=pyhd3eb1b0_0\n",
            "    - pyopenssl==22.0.0=pyhd3eb1b0_0\n",
            "    - pysocks==1.7.1=py38h06a4308_0\n",
            "    - python==3.8.13=h12debd9_0\n",
            "    - readline==8.1.2=h7f8727e_1\n",
            "    - requests==2.27.1=pyhd3eb1b0_0\n",
            "    - ruamel_yaml==0.15.100=py38h27cfd23_0\n",
            "    - setuptools==61.2.0=py38h06a4308_0\n",
            "    - six==1.16.0=pyhd3eb1b0_1\n",
            "    - sqlite==3.38.2=hc218d9a_0\n",
            "    - tk==8.6.11=h1ccaba5_0\n",
            "    - tqdm==4.63.0=pyhd3eb1b0_0\n",
            "    - urllib3==1.26.8=pyhd3eb1b0_0\n",
            "    - wheel==0.37.1=pyhd3eb1b0_0\n",
            "    - xz==5.2.5=h7b6447c_0\n",
            "    - yaml==0.2.5=h7b6447c_0\n",
            "    - zlib==1.2.12=h7f8727e_1\n",
            "\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  _libgcc_mutex      pkgs/main/linux-64::_libgcc_mutex-0.1-main\n",
            "  _openmp_mutex      pkgs/main/linux-64::_openmp_mutex-4.5-1_gnu\n",
            "  brotlipy           pkgs/main/linux-64::brotlipy-0.7.0-py38h27cfd23_1003\n",
            "  ca-certificates    pkgs/main/linux-64::ca-certificates-2022.3.29-h06a4308_1\n",
            "  certifi            pkgs/main/linux-64::certifi-2021.10.8-py38h06a4308_2\n",
            "  cffi               pkgs/main/linux-64::cffi-1.15.0-py38hd667e15_1\n",
            "  charset-normalizer pkgs/main/noarch::charset-normalizer-2.0.4-pyhd3eb1b0_0\n",
            "  colorama           pkgs/main/noarch::colorama-0.4.4-pyhd3eb1b0_0\n",
            "  conda              pkgs/main/linux-64::conda-4.12.0-py38h06a4308_0\n",
            "  conda-content-tru~ pkgs/main/noarch::conda-content-trust-0.1.1-pyhd3eb1b0_0\n",
            "  conda-package-han~ pkgs/main/linux-64::conda-package-handling-1.8.1-py38h7f8727e_0\n",
            "  cryptography       pkgs/main/linux-64::cryptography-36.0.0-py38h9ce1e76_0\n",
            "  idna               pkgs/main/noarch::idna-3.3-pyhd3eb1b0_0\n",
            "  ld_impl_linux-64   pkgs/main/linux-64::ld_impl_linux-64-2.35.1-h7274673_9\n",
            "  libffi             pkgs/main/linux-64::libffi-3.3-he6710b0_2\n",
            "  libgcc-ng          pkgs/main/linux-64::libgcc-ng-9.3.0-h5101ec6_17\n",
            "  libgomp            pkgs/main/linux-64::libgomp-9.3.0-h5101ec6_17\n",
            "  libstdcxx-ng       pkgs/main/linux-64::libstdcxx-ng-9.3.0-hd4cf53a_17\n",
            "  ncurses            pkgs/main/linux-64::ncurses-6.3-h7f8727e_2\n",
            "  openssl            pkgs/main/linux-64::openssl-1.1.1n-h7f8727e_0\n",
            "  pip                pkgs/main/linux-64::pip-21.2.4-py38h06a4308_0\n",
            "  pycosat            pkgs/main/linux-64::pycosat-0.6.3-py38h7b6447c_1\n",
            "  pycparser          pkgs/main/noarch::pycparser-2.21-pyhd3eb1b0_0\n",
            "  pyopenssl          pkgs/main/noarch::pyopenssl-22.0.0-pyhd3eb1b0_0\n",
            "  pysocks            pkgs/main/linux-64::pysocks-1.7.1-py38h06a4308_0\n",
            "  python             pkgs/main/linux-64::python-3.8.13-h12debd9_0\n",
            "  readline           pkgs/main/linux-64::readline-8.1.2-h7f8727e_1\n",
            "  requests           pkgs/main/noarch::requests-2.27.1-pyhd3eb1b0_0\n",
            "  ruamel_yaml        pkgs/main/linux-64::ruamel_yaml-0.15.100-py38h27cfd23_0\n",
            "  setuptools         pkgs/main/linux-64::setuptools-61.2.0-py38h06a4308_0\n",
            "  six                pkgs/main/noarch::six-1.16.0-pyhd3eb1b0_1\n",
            "  sqlite             pkgs/main/linux-64::sqlite-3.38.2-hc218d9a_0\n",
            "  tk                 pkgs/main/linux-64::tk-8.6.11-h1ccaba5_0\n",
            "  tqdm               pkgs/main/noarch::tqdm-4.63.0-pyhd3eb1b0_0\n",
            "  urllib3            pkgs/main/noarch::urllib3-1.26.8-pyhd3eb1b0_0\n",
            "  wheel              pkgs/main/noarch::wheel-0.37.1-pyhd3eb1b0_0\n",
            "  xz                 pkgs/main/linux-64::xz-5.2.5-h7b6447c_0\n",
            "  yaml               pkgs/main/linux-64::yaml-0.2.5-h7b6447c_0\n",
            "  zlib               pkgs/main/linux-64::zlib-1.2.12-h7f8727e_1\n",
            "\n",
            "\n",
            "Preparing transaction: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Executing transaction: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "installation finished.\n",
            "WARNING:\n",
            "    You currently have a PYTHONPATH environment variable set. This may cause\n",
            "    unexpected behavior when running the Python interpreter in Miniconda3.\n",
            "    For best results, please verify that your PYTHONPATH only points to\n",
            "    directories of packages that are compatible with the Python interpreter\n",
            "    in Miniconda3: /usr/local\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version\n",
        "sys.version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "3qJa6AQ-S4t3",
        "outputId": "c0449644-27d0-44ce-b491-b996b5e441f0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.8.13\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3.7.14 (default, Sep  8 2022, 00:06:44) \\n[GCC 7.5.0]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-vn_F0mmmG2"
      },
      "source": [
        "### Install Libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -V"
      ],
      "metadata": {
        "id": "JW_JU5rMZNQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sysconfig; print(sysconfig.get_paths()[\"purelib\"])"
      ],
      "metadata": {
        "id": "OErGhaK0Zq0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m pip install numpyro\n",
        "import numpyro"
      ],
      "metadata": {
        "id": "WdrGVN8jXGzv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gmfYY3UIaKxZ"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade numpy\n",
        "!pip install --upgrade pandas\n",
        "!pip install --upgrade pymc\n",
        "!pip install --upgrade numpyro\n",
        "!pip install --upgrade blackjax\n",
        "!pip install --upgrade pint_xarray\n",
        "!pip install --upgrade bottleneck\n",
        "!pip install --upgrade numbagg\n",
        "!pip install --upgrade ipython-autotime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwuQ_oVXgYW1"
      },
      "source": [
        "### Setup Platform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qNXwSpAYe-Y0"
      },
      "outputs": [],
      "source": [
        "import numpyro\n",
        "import jax\n",
        "\n",
        "try:\n",
        "    #import jax.tools.colab_tpu\n",
        "    \n",
        "    jax.tools.colab_tpu.setup_tpu()\n",
        "    numpyro.util.set_platform('tpu')\n",
        "\n",
        "    print('TPU assigned.')\n",
        "    display(jax.local_devices())\n",
        "\n",
        "except:\n",
        "    print('No TPU available, trying GPU.')\n",
        "    \n",
        "    try:\n",
        "        numpyro.set_platform('gpu')\n",
        "        print('GPU assigned.')\n",
        "        display(jax.local_devices())\n",
        "\n",
        "    except:\n",
        "        numpyro.set_platform('cpu')\n",
        "        print('No GPU available, using CPU.')\n",
        "        display(jax.local_devices())\n",
        "        \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVaqZX-3OTWl"
      },
      "source": [
        "### Load Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9R2YmMNmPQFn"
      },
      "outputs": [],
      "source": [
        "%load_ext autotime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ohaxLYsbf4e2"
      },
      "outputs": [],
      "source": [
        "import pymc as pm\n",
        "import pymc.sampling_jax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R43yCeSDP5BD"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import pint_xarray\n",
        "import warnings\n",
        "\n",
        "import requests\n",
        "import numpyro\n",
        "import sklearn.preprocessing\n",
        "import sklearn.metrics\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import arviz as az\n",
        "import xarray as xr\n",
        "\n",
        "from jax import numpy as jnp\n",
        "from numpyro import distributions as dist\n",
        "\n",
        "from collections import OrderedDict\n",
        "from google.colab import drive\n",
        "from requests import get\n",
        "from urllib.parse import unquote\n",
        "\n",
        "from requests import get\n",
        "from urllib.parse import unquote\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from typing import Callable, Optional, Dict, List, Union, NoReturn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PQgvlBLOjTS"
      },
      "source": [
        "### Set Global Vars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q315qyvjA7lm"
      },
      "outputs": [],
      "source": [
        "rng_key = jax.random.PRNGKey(33)\n",
        "n_devices = len(jax.local_devices())\n",
        "numpyro.set_host_device_count(n_devices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hCopJyB9oteF"
      },
      "outputs": [],
      "source": [
        "plt.style.use('bmh')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3Rq7AHHnJNT"
      },
      "outputs": [],
      "source": [
        "csv_path = 'https://raw.githubusercontent.com/mattiasthalen/Bayesian-Velocity-Profiling/main/RepOne_Data_Export.csv'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blDhXZLZQ4BB"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXNFV7da7HqZ"
      },
      "source": [
        "### Regression Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8cBkPiSVrBW"
      },
      "outputs": [],
      "source": [
        "def calc_obs_weight(mvt, velocity, weight, max_weight):\n",
        "    try:\n",
        "        mvt = mvt.pint.dequantify()\n",
        "        velocity = velocity.pint.dequantify()\n",
        "        weight = weight.pint.dequantify()\n",
        "        max_weight = max_weight.pint.dequantify()\n",
        "    except:\n",
        "        pass\n",
        "    finally:\n",
        "        obs_weight = 0.5**((1 - mvt/velocity)/0.2) * 0.5**((1 - weight/max_weight)/0.2)\n",
        "        \n",
        "    return obs_weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uB8AOtcGnbkj"
      },
      "outputs": [],
      "source": [
        "def linear_fn(x, intercept, slope):\n",
        "    return intercept + slope*x\n",
        "\n",
        "def linear_rev_fn(x, intercept, slope):\n",
        "    return (x - intercept)/slope\n",
        "\n",
        "def linear_fit(ds, var, coord, reduce_dims):\n",
        "    ds = ds.where((ds.coords['set_type'] != 'Back Off') & (ds['workup_sets'] > 2), drop = True)\\\n",
        "              .sel({'aggregation': 'max'}, drop = True)\\\n",
        "              .pint.dequantify()\n",
        "\n",
        "    coefs = ds[var].curvefit(coords = ds[coord],\n",
        "                             func = linear_fn,\n",
        "                             reduce_dims = reduce_dims)\\\n",
        "                   .rename({'param': 'regression_param'})\n",
        "    \n",
        "    return coefs['curvefit_coefficients']\n",
        "\n",
        "def linear_predict(x, coefs, reverse = False):\n",
        "    intercept = coefs.sel(regression_param = 'intercept', drop = True)\n",
        "    slope = coefs.sel(regression_param = 'slope', drop = True)\n",
        "\n",
        "    if reverse:\n",
        "            return linear_rev_fn(x, intercept, slope)\n",
        "\n",
        "    return linear_fn(x, intercept, slope)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdWQyE4gQ8BF"
      },
      "source": [
        "### Plotting Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMHAdbMtRBdk"
      },
      "outputs": [],
      "source": [
        "def plot_pbc(ds, exercise, data_var, window = 20, signal_window = 8, ax = None):\n",
        "    df = ds[data_var].sel({'exercise': exercise}, drop = True)\\\n",
        "                     .drop_vars(['training_cycle', 'cycle_type', 'max_weight_pr_flag'])\\\n",
        "                     .to_dataframe()\\\n",
        "                     .dropna()\n",
        "    \n",
        "    df['moving_average'] = df[data_var].sort_index(ascending = False)\\\n",
        "                                       .rolling(window, min_periods = 1)\\\n",
        "                                       .mean()\n",
        "    \n",
        "    df['moving_range'] = df[data_var].diff(-1)\\\n",
        "                                     .abs()\\\n",
        "                                     .sort_index(ascending = False)\\\n",
        "                                     .rolling(window, min_periods = 1)\\\n",
        "                                     .mean()\n",
        "\n",
        "    df['process_average'] = df['moving_average']\n",
        "    df['process_range'] = df['moving_range']\n",
        "    df['signal'] = None\n",
        "    df['signal_min'] = None\n",
        "    df['signal_max'] = None\n",
        "    df['signal_above_average'] = None\n",
        "    df['signal_below_average'] = None\n",
        "\n",
        "    n_rows = len(df)\n",
        "    previous_signal_id = 0\n",
        "\n",
        "    for row in np.arange(n_rows):\n",
        "        first_row = row == 0\n",
        "        sufficient_rows_left = n_rows - row >= window\n",
        "\n",
        "        signal_start_id = np.max([8, row - signal_window])\n",
        "\n",
        "        df['signal_min'][row] = df[data_var][signal_start_id:row].min()\n",
        "        df['signal_max'][row] = df[data_var][signal_start_id:row].max()\n",
        "\n",
        "        df['signal_above_average'][row] = (df['signal_min'][row] > df['process_average'][row - 1])\n",
        "        df['signal_below_average'][row] = (df['signal_max'][row] < df['process_average'][row - 1])\n",
        "\n",
        "        signal_open = (first_row) | (row >= previous_signal_id + window)\n",
        "        signal = (signal_open) & (sufficient_rows_left) & (first_row | df['signal_above_average'][row] | df['signal_below_average'][row])\n",
        "        df['signal'][row] = signal\n",
        "        \n",
        "        df['process_average'][row] =  df['process_average'][row - 1]\n",
        "        df['process_range'][row] =  df['process_range'][row - 1]\n",
        "\n",
        "        if signal:\n",
        "            previous_signal_id = row\n",
        "            df['process_average'][row] =  df['moving_average'][row]\n",
        "            df['process_range'][row] =  df['moving_range'][row]\n",
        "        else:\n",
        "            df['process_average'][row] =  df['process_average'][row - 1]\n",
        "            df['process_range'][row] =  df['process_range'][row - 1]\n",
        "\n",
        "    df['lower_limit_1'] = df['process_average'] - df['process_range']/1.128\n",
        "    df['upper_limit_1'] = df['process_average'] + df['process_range']/1.128\n",
        "    df['lower_limit_2'] = df['process_average'] - df['process_range']*2/1.128\n",
        "    df['upper_limit_2'] = df['process_average'] + df['process_range']*2/1.128\n",
        "    df['lower_limit_3'] = df['process_average'] - df['process_range']*3/1.128\n",
        "    df['upper_limit_3'] = df['process_average'] + df['process_range']*3/1.128\n",
        "\n",
        "    if ax is None:\n",
        "        ax = plt.gca()\n",
        "\n",
        "    ax.scatter(df.index, df[data_var], marker = '.', alpha = 0.6)\n",
        "    ax.plot(df.index, df['process_average'])\n",
        "    ax.plot(df.index, df['lower_limit_3'])\n",
        "    ax.plot(df.index, df['upper_limit_3'])\n",
        "\n",
        "    ax.fill_between(df.index,df['lower_limit_1'], df['upper_limit_1'], alpha = 0.3)\n",
        "\n",
        "    ax.set_xlabel('')\n",
        "    ax.set_ylabel('')\n",
        "    #ax.set_ylim([0, None])\n",
        "    ax.tick_params(labelrotation = 90)\n",
        "    ax.grid()\n",
        "\n",
        "    return ax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rTUijXu0i27"
      },
      "outputs": [],
      "source": [
        "def plot_kpis(ds, exercise, vars):\n",
        "    var_titles = [var.title().replace('_', ' ').replace('1Rm', '1RM') for var in vars]\n",
        "\n",
        "    n_vars = len(vars)\n",
        "\n",
        "    n_cols = 1\n",
        "\n",
        "    if n_vars > 1:\n",
        "        n_cols = 2\n",
        "    \n",
        "    n_rows = 1\n",
        "\n",
        "    if n_vars > 2:\n",
        "        n_rows = np.ceil(n_vars/n_cols).astype(int)\n",
        "    \n",
        "    figsize = np.array([6, 3]) * [n_cols, n_rows]\n",
        "\n",
        "    fig, axes = plt.subplots(ncols = n_cols,\n",
        "                            nrows = n_rows,\n",
        "                            constrained_layout = True,\n",
        "                            figsize = figsize,\n",
        "                            sharex = True)\n",
        "    \n",
        "    axes = [ax for row in axes for ax in row]\n",
        "\n",
        "    for key, val in enumerate(vars):\n",
        "        plot_pbc(ds, exercise, vars[key], ax = axes[key])\n",
        "        title = var_titles[key]\n",
        "        axes[key].set_title(title)\n",
        "\n",
        "    fig.suptitle(f'{exercise.title()} KPIs', fontsize = 16)\n",
        "    fig.supxlabel('Workout Start Time')\n",
        "\n",
        "    plt.draw()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubofKRwqoci-"
      },
      "source": [
        "### ETL Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qgN4QogIocFZ"
      },
      "outputs": [],
      "source": [
        "def new_extract(csv_path, ds_path, **kwargs):\n",
        "    try:\n",
        "        return os.path.getmtime(csv_path) > os.path.getmtime(ds_path)\n",
        "    except:\n",
        "        return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nyqfPoReqF_Y"
      },
      "outputs": [],
      "source": [
        "def extract_data(csv_path, **kwargs):\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    df.columns = (df.columns\n",
        "                    .str.lower()\n",
        "                    .str.replace(' \\(m/s\\)', '')\n",
        "                    .str.replace(' \\(mm\\)', '')\n",
        "                    .str.replace(' \\(sec\\)', '')\n",
        "                    .str.replace(' \\(%\\)', '')\n",
        "                    .str.replace(' ', '_'))\n",
        "    \n",
        "    df.rename(columns = {'weight': 'load'}, inplace = True)\n",
        "\n",
        "    df['workout_start_time'] = pd.to_datetime(df['workout_start_time'], format = '%d/%m/%Y, %H:%M:%S')\n",
        "\n",
        "    df.dropna(subset = ['exercise'], inplace = True)\n",
        "    df['rest_time'] = pd.to_timedelta(df['rest_time'])\n",
        "\n",
        "    # Correct split session\n",
        "    df['set'].mask((df['exercise'] == 'deadlift') & (df['workout_start_time'] == pd.to_datetime('2020-12-30 13:06:04')), df['set'] + 7, inplace = True)\n",
        "    df.replace({'workout_start_time': pd.to_datetime('2020-12-30 13:06:04')}, pd.to_datetime('2020-12-30 12:53:09'), inplace = True)\n",
        "    df.replace({'workout_start_time': pd.to_datetime('2021-01-07 11:50:22')}, pd.to_datetime('2021-01-07 11:20:07'), inplace = True)\n",
        "    df.replace({'workout_start_time': pd.to_datetime('2021-06-10 12:02:22')}, pd.to_datetime('2021-06-10 11:56:31'), inplace = True)\n",
        "    df.replace({'workout_start_time': pd.to_datetime('2021-06-14 12:06:00')}, pd.to_datetime('2021-06-14 11:57:50'), inplace = True)\n",
        "\n",
        "    # Reindex sets & reps to counter bugs in the extract\n",
        "    df['set'] = df.groupby(['exercise', 'workout_start_time'])['set'].apply(lambda x: (x != x.shift()).cumsum() - 1)\n",
        "    df['rep'] = df.groupby(['exercise', 'workout_start_time', 'set']).cumcount()\n",
        "\n",
        "    # Convert from , to . as decimal sign\n",
        "    df['load'] = df['load'].str.replace(',', '.').astype('float')\n",
        "\n",
        "    # Drop rows with tag fail\n",
        "    fail_filter = df['tags'].str.contains('fail', na = False)\n",
        "    df = df[~fail_filter]\n",
        "\n",
        "    # Handle the case when a rep is split into two reps\n",
        "    rep_split_filter = df['tags'].str.contains('rep split', na = False)\n",
        "\n",
        "    rep_split_df = df[rep_split_filter].groupby(['exercise', 'workout_start_time', 'set', 'load', 'metric'])[['range_of_motion', 'duration_of_rep']].sum()\n",
        "    rep_split_df['avg_velocity'] = rep_split_df['range_of_motion']/1000/rep_split_df['duration_of_rep']\n",
        "    rep_split_df['rep'] = 0\n",
        "    rep_split_df.reset_index(inplace = True)\n",
        "\n",
        "    rep_split_df = rep_split_df.groupby(['exercise', 'workout_start_time', 'set', 'rep']).max()\n",
        "\n",
        "    df = pd.concat([df[~rep_split_filter], rep_split_df])\n",
        "\n",
        "    # Group to get multi index\n",
        "    df = df.groupby(['exercise', 'workout_start_time', 'set', 'rep']).max()\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xz8urMMhrcIV"
      },
      "outputs": [],
      "source": [
        "def define_cycles(cycles, min_workout_start_time, **kwargs):\n",
        "    cycle_start_dates = np.array(list(cycles.keys())).astype('datetime64[ns]')\n",
        "\n",
        "    if min_workout_start_time < cycle_start_dates.min():\n",
        "        cycles = {min_workout_start_time.strftime('%Y-%m-%d'): 'n/a', **cycles}\n",
        "        cycle_start_dates = np.array(list(cycles.keys())).astype('datetime64[ns]')\n",
        "\n",
        "    cycle_types = np.array(list(cycles.values()))\n",
        "    cycle_id = np.arange(len(cycle_start_dates))\n",
        "\n",
        "    ds = xr.Dataset(coords = {'workout_start_time': cycle_start_dates,\n",
        "                              'training_cycle': ('workout_start_time', cycle_id),\n",
        "                              'cycle_type': ('workout_start_time', cycle_types)})\n",
        "\n",
        "    return ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kdOFZqLRtlZp"
      },
      "outputs": [],
      "source": [
        "def transform_data(df, **kwargs):\n",
        "    # Convert to xarray\n",
        "    ds = df.to_xarray()\n",
        "\n",
        "    # Change Set and Rep to integers\n",
        "    ds['set'] = ds['set'].astype(int)\n",
        "    ds['rep'] = ds['rep'].astype(int)\n",
        "\n",
        "    # Move variables to coords\n",
        "    ds = ds.set_coords(['metric', 'tags'])\n",
        "\n",
        "    # Define UOMs\n",
        "    ds = ds.pint.quantify({'load': 'kg',\n",
        "                           'avg_velocity': 'meter / second',\n",
        "                           'peak_velocity': 'meter / seconds',\n",
        "                           'range_of_motion': 'mm',\n",
        "                           'duration_of_rep': 's'})\n",
        "\n",
        "    # Session meta data\n",
        "    session_stack = ['exercise', 'workout_start_time']\n",
        "    ds['session_max_load'] = ds['load'].stack(stack = session_stack)\\\n",
        "                                       .groupby('stack')\\\n",
        "                                       .reduce(all_nan_max, ...)\\\n",
        "                                       .unstack()\n",
        "\n",
        "    # Set meta data\n",
        "    set_stack = ['exercise', 'workout_start_time', 'set']\n",
        "    ds['load'] = (ds['load'].stack(stack = set_stack)\n",
        "                            .groupby('stack')\n",
        "                            .reduce(all_nan_max, ...)\n",
        "                            .unstack())\n",
        "\n",
        "    ds['reps'] = (ds['avg_velocity'].stack(stack = set_stack)\n",
        "                                    .groupby('stack')\n",
        "                                    .count(...)\n",
        "                                    .unstack()\n",
        "                                    .where(ds['load'] > 0, drop = True))\n",
        "\n",
        "    ds['set_velocities'] = summarize(ds['avg_velocity'].pint.dequantify())\n",
        "    ds['set_velocities'] = ds['set_velocities'].pint.quantify({ds['set_velocities'].name: 'mps'})\n",
        "\n",
        "    ds.coords['set_type'] = assign_set_type(ds['load'])\n",
        "\n",
        "    # Merge training cycle coordinates\n",
        "    cycles = define_cycles(**kwargs)\n",
        "    ds = ds.merge(cycles, join = 'outer')\n",
        "    ds['cycle_type'] = ('workout_start_time', ds['cycle_type'].to_series().ffill())\n",
        "    ds['training_cycle'] = ds['training_cycle'].ffill('workout_start_time').astype(int)\n",
        "\n",
        "    # Add the running min top set velocity per exercise\n",
        "    ds['minimum_velocity_threshold'] = (ds['set_velocities'].sel({'aggregation': 'first'})\n",
        "                                                            .where(ds.coords['set_type'] == 'Top Set')\n",
        "                                                            .pint.dequantify()\n",
        "                                                            .stack(stack = ['exercise', 'workout_start_time'])\n",
        "                                                            .groupby('stack')\n",
        "                                                            .reduce(all_nan_min, ...)\n",
        "                                                            .unstack()\n",
        "                                                            .rolling({'workout_start_time': len(ds['workout_start_time'])},\n",
        "                                                                    min_periods = 1)\n",
        "                                                            .min())\n",
        "    ds['minimum_velocity_threshold'] = ds['minimum_velocity_threshold'].pint.quantify({ds['minimum_velocity_threshold'].name: 'meter / second'})\n",
        "\n",
        "    # Add running max load per exercise\n",
        "    ds['rolling_max_load'] = (ds['load'].pint.dequantify()\n",
        "                                             .stack(stack = ['exercise', 'workout_start_time'])\n",
        "                                             .groupby('stack')\n",
        "                                             .reduce(all_nan_max, ...)\n",
        "                                             .unstack()\n",
        "                                             .rolling({'workout_start_time': len(ds['workout_start_time'])},\n",
        "                                                      min_periods = 1)\n",
        "                                             .max())\n",
        "    ds['rolling_max_load'] = ds['rolling_max_load'].pint.quantify({ds['rolling_max_load'].name: 'kg'})\n",
        "\n",
        "    # Generate the observation weights\n",
        "    ds['observation_weight'] = calc_obs_weight(ds['minimum_velocity_threshold'], ds['set_velocities'], ds['load'], ds['session_max_load'])\n",
        "\n",
        "    # Additional session meta data\n",
        "    ds['workup_sets'] = ds['load'].where(ds.coords['set_type'] == 'Work Up', drop = True)\\\n",
        "                                  .stack(stack = session_stack)\\\n",
        "                                  .groupby('stack')\\\n",
        "                                  .count(...)\\\n",
        "                                  .unstack()\n",
        "    \n",
        "    ds['session_regression_coefficients'] = linear_fit(ds, 'load', 'set_velocities', 'set')\n",
        "\n",
        "    ds['estimated_1rm'] = linear_predict(ds['minimum_velocity_threshold'].pint.dequantify(), ds['session_regression_coefficients'])\n",
        "    ds['estimated_1rm'] = ds['estimated_1rm'].pint.quantify({ds['estimated_1rm'].name: 'kg'})\n",
        "\n",
        "    ds['zero_velocity_load'] = linear_predict(0, ds['session_regression_coefficients'])\n",
        "    ds['zero_velocity_load'] = ds['zero_velocity_load'].pint.quantify({ds['zero_velocity_load'].name: 'kg'})\n",
        "\n",
        "    ds['zero_load_velocity'] = linear_predict(0, ds['session_regression_coefficients'], reverse = True)\n",
        "    ds['zero_load_velocity'] = ds['zero_load_velocity'].pint.quantify({ds['zero_load_velocity'].name: 'mps'})\n",
        "\n",
        "    ds['curve_score'] = ds['zero_velocity_load'].pint.dequantify()*ds['zero_load_velocity'].pint.dequantify()/2\n",
        "\n",
        "    ds['session_volume'] = (ds['load'] * ds['reps']).stack(stack = session_stack).groupby('stack').sum(...).unstack()\n",
        "    ds['session_relative_volume'] = ds['session_volume']/ds['estimated_1rm']\n",
        "\n",
        "    # Rep meta data\n",
        "    ds['rep_exertion'] = linear_predict(ds['avg_velocity'].pint.dequantify(), ds['session_regression_coefficients'])/ds['estimated_1rm'].pint.dequantify()\n",
        "    ds['rep_force'] = (ds['load']*ds['range_of_motion'].pint.to('meter')/ds['duration_of_rep']**2).pint.to('N')\n",
        "    ds['rep_energy'] = (ds['rep_force']*ds['range_of_motion'].pint.to('meter')).pint.to('J')\n",
        "\n",
        "    # Session meta data\n",
        "    ds['session_exertion_load'] = ds['rep_exertion'].stack(stack = ['exercise', 'workout_start_time']).groupby('stack').reduce(all_nan_sum, ...).unstack().pint.dequantify()\n",
        "\n",
        "    # Add PR coordinates\n",
        "    ds.coords['max_load_pr_flag'] = ds['rolling_max_load'].diff('workout_start_time') > 0\n",
        "\n",
        "    return ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVUixL-apqjH"
      },
      "outputs": [],
      "source": [
        "def etl_data(**kwargs):\n",
        "    df = extract_data(**kwargs)\n",
        "\n",
        "    min_workout_start_time = df.index.get_level_values('workout_start_time').min().floor('D')\n",
        "\n",
        "    ds = transform_data(df, min_workout_start_time = min_workout_start_time, **kwargs)\n",
        "\n",
        "    return ds\n",
        "    \n",
        "    try:\n",
        "        os.remove(ds_path)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    try:\n",
        "        ds.to_netcdf(ds_path)\n",
        "    except PermissionError:\n",
        "        print('PermissionError')    \n",
        "\n",
        "    return ds   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONhxw8JIpB9A"
      },
      "outputs": [],
      "source": [
        "def get_data(**kwargs):\n",
        "    #if not new_extract(**kwargs):\n",
        "    #    return xr.open_dataset(kwargs['ds_path'])\n",
        "    \n",
        "    return etl_data(**kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_PgwhIy7mF0"
      },
      "source": [
        "### Miscellaneous Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tnyih5CgK4EY"
      },
      "outputs": [],
      "source": [
        "def assign_set_type(da):\n",
        "    set_category = xr.where(da['set'] < da.idxmax('set'), 'Work Up', np.nan)\n",
        "    set_category = xr.where(da['set'] == da.idxmax('set'), 'Top Set', set_category)\n",
        "    set_category = xr.where(da['set'] > da.idxmax('set'), 'Back Off', set_category)\n",
        "    set_category = xr.where(np.isnan(da), np.nan, set_category)\n",
        "    \n",
        "    return set_category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FV50KZquxdj5"
      },
      "outputs": [],
      "source": [
        "def agg_summarize(x):\n",
        "    x = x[np.isfinite(x)]\n",
        "\n",
        "    if len(x) > 0:\n",
        "        min = np.min(x)\n",
        "        max = np.max(x)\n",
        "        first = x[0]\n",
        "        last = x[-1]\n",
        "        peak_end = np.mean([min, last])\n",
        "        mean = np.mean(x)\n",
        "        median = np.median(x)\n",
        "        hdi = az.hdi(x)\n",
        "        result = np.array([min, max, first, last, peak_end, mean, median, *hdi])\n",
        "    else:\n",
        "        result = np.array([np.nan]*9)\n",
        "    \n",
        "    return result\n",
        "\n",
        "def summarize(x, reduce_dim = 'rep'):\n",
        "    summaries = xr.apply_ufunc(agg_summarize,\n",
        "                               x,\n",
        "                               vectorize = True,\n",
        "                               input_core_dims = [[reduce_dim]],\n",
        "                               output_core_dims = [['aggregation']])\n",
        "    \n",
        "    summaries['aggregation'] = ['min', 'max', 'first', 'last', 'peak_end', 'mean', 'median', 'hdi_lower', 'hdi_upper']\n",
        "    \n",
        "    return summaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTGyzLdT0s5G"
      },
      "outputs": [],
      "source": [
        "def agg_hdi_summary(x):\n",
        "    mean = x.mean()\n",
        "    median = np.median(x)\n",
        "    hdi = az.hdi(x)\n",
        "\n",
        "    return np.array([mean, median, *hdi])\n",
        "\n",
        "def hdi_summary(x, reduce_dim = 'sample'):\n",
        "    try:\n",
        "        x = x.pint.dequantify()\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    summaries = xr.apply_ufunc(agg_hdi_summary,\n",
        "                               x,\n",
        "                               vectorize = True,\n",
        "                               input_core_dims = [[reduce_dim]],\n",
        "                               output_core_dims = [['hdi_aggregation']])\n",
        "    \n",
        "    summaries['hdi_aggregation'] = ['mean', 'median', 'hdi_lower', 'hdi_upper']\n",
        "    \n",
        "    return summaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gisKtoreeVSI"
      },
      "outputs": [],
      "source": [
        "def all_nan_summary(x, mode = 'mean', **kwargs):\n",
        "    if np.all(np.isnan(x)):\n",
        "        return np.nan\n",
        "    elif mode == 'max':\n",
        "        return np.nanmax(x)\n",
        "    elif mode == 'min':\n",
        "        return np.nanmin(x)\n",
        "    elif mode == 'mean':\n",
        "        return np.nanmean(x)\n",
        "    elif mode == 'sum':\n",
        "        return np.nansum(x)\n",
        "        \n",
        "def all_nan_max(x, **kwargs):\n",
        "    return all_nan_summary(x, 'max', **kwargs)\n",
        "        \n",
        "def all_nan_min(x, **kwargs):\n",
        "    return all_nan_summary(x, 'min', **kwargs)\n",
        "        \n",
        "def all_nan_mean(x, **kwargs):\n",
        "    return all_nan_summary(x, 'mean', **kwargs)\n",
        "        \n",
        "def all_nan_sum(x, **kwargs):\n",
        "    return all_nan_summary(x, 'sum', **kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqmRu_8pOLt4"
      },
      "outputs": [],
      "source": [
        "def profile(data, exercise, ds, mvt = None, plot = True, use_weights = True, **kwargs):\n",
        "    tic = time.perf_counter()\n",
        "\n",
        "    data = np.array(list(data.items()))\n",
        "    \n",
        "    load = data[:, 0]\n",
        "    velocity = data[:, 1]\n",
        "    \n",
        "    # Set minimum velocity threshold\n",
        "    if mvt is None:        \n",
        "        try:\n",
        "            mvt = ds['minimum_velocity_threshold'].sel({'exercise': exercise.lower()})[-1]\n",
        "        except:\n",
        "            mvt = mvt.mean()\n",
        "\n",
        "        try:\n",
        "            mvt = mvt.pint.dequantify().item()\n",
        "        except:\n",
        "            mvt = mvt.item()\n",
        "    \n",
        "    mvt = np.nanmin([mvt, velocity.min()])\n",
        "\n",
        "    # Max load\n",
        "    try:\n",
        "        max_load = ds['rolling_max_weight'].sel({'exercise': exercise.lower()})[-1]\n",
        "        try:\n",
        "            max_load = max_load.pint.dequantify().item()\n",
        "        except:\n",
        "            max_load = max_load.item()\n",
        "    except:\n",
        "        max_load = np.nan\n",
        "    \n",
        "    max_load = np.nanmax([max_load, load.max()])\n",
        "    \n",
        "    if use_weights:\n",
        "        weight = calc_obs_weight(mvt, velocity, load, max_load)\n",
        "    else:\n",
        "        weight = np.ones(len(load))\n",
        "\n",
        "    #inference_data = quadratic_fit(load, velocity, weight, **kwargs)\n",
        "\n",
        "    #if plot:\n",
        "    #    plot_profile(inference_data, mvt, exercise, **kwargs)\n",
        "\n",
        "    toc = time.perf_counter()\n",
        "    print(f'Profile completed in {toc - tic:0.4f} seconds.')\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBrZ4RuhqIdC"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SovWMw2MtZR6"
      },
      "outputs": [],
      "source": [
        "cycles = {'2021-01-05': '95% x1 + 75% x6+ @ 90%',\n",
        "          '2021-01-27': '95% x1',\n",
        "          '2021-02-01': '95% x1 + 75% x3+ @ 80%',\n",
        "          '2021-02-15': '95% x1 + 80% x1 @ 85%',\n",
        "          '2021-03-15': '95% x1 + 75% x6[3] @ 90%',\n",
        "          '2021-03-24': '95% x1 + 75% x3 @ 90%',\n",
        "          '2021-04-10': '95% x1 + 75% x2-3 @ 90-95% [320]',\n",
        "          '2021-04-13': '95% x1 + 80% x2 @ 95% [320]',\n",
        "          '2021-04-20': '95% x1',\n",
        "          '2021-06-01': '95% x1 + 85% x2',\n",
        "          '2021-07-06': 'YOLO',\n",
        "          }\n",
        "\n",
        "data = get_data(csv_path = csv_path,\n",
        "              cycles = cycles)\n",
        "\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-jkSE9yxipd"
      },
      "outputs": [],
      "source": [
        "_ = (data.pint.dequantify()\n",
        "         .plot.scatter(x = 'workout_start_time',\n",
        "                       y = 'session_max_load',\n",
        "                       hue = 'exercise',\n",
        "                       marker = '.',\n",
        "                       add_guide = False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8G37MnsGlipD"
      },
      "source": [
        "### Prepare Data For Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pR5KAgvkBCs4"
      },
      "outputs": [],
      "source": [
        "regr_data = (data[['load', 'set_velocities', 'observation_weight']]\n",
        "             .pint.dequantify()\n",
        "             .sel({'aggregation': 'max'}, drop = True)\n",
        "             .where(data.coords['set_type'] != 'Back Off')\n",
        "             .where(data.coords['exercise'] != 'front squat')\n",
        "             .drop_vars(['set_type', 'training_cycle', 'max_load_pr_flag'])\n",
        "             .to_dataframe()\n",
        "             .dropna()\n",
        "             .reset_index()\n",
        "             .sort_values(by = ['workout_start_time', 'exercise', 'set'])\n",
        "             .rename(columns = {'set_velocities': 'velocity'}))\n",
        "\n",
        "load_scaler = sklearn.preprocessing.StandardScaler()\n",
        "regr_data['load_scaled'] = load_scaler.fit_transform(regr_data['load'].values.reshape(-1, 1))\n",
        "\n",
        "velocity_scaler = sklearn.preprocessing.StandardScaler()\n",
        "regr_data['velocity_scaled'] = velocity_scaler.fit_transform(regr_data['velocity'].values.reshape(-1, 1))\n",
        "\n",
        "exercise_encoder = sklearn.preprocessing.LabelEncoder()\n",
        "regr_data['exercise_id'] = exercise_encoder.fit_transform(regr_data['exercise'].values)\n",
        "\n",
        "regr_data['session_id'] = regr_data.groupby(['exercise', 'workout_start_time'], sort = False).ngroup()\n",
        "\n",
        "regr_data['observation'] = np.arange(len(regr_data))\n",
        "regr_data.set_index('observation', inplace = True)\n",
        "\n",
        "regr_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G21clwu5uzFn"
      },
      "outputs": [],
      "source": [
        "map_session_to_exercise_id = (regr_data[['session_id', 'exercise_id']]\n",
        "                              .drop_duplicates()\n",
        "                              .set_index('session_id', verify_integrity = True)\n",
        "                              .sort_index()['exercise_id']\n",
        "                              .values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "807i_I2KxLcB"
      },
      "outputs": [],
      "source": [
        "data_dict = dict({\n",
        "    'velocity_std': regr_data['velocity_scaled'].values,\n",
        "    'load_std': regr_data['load_scaled'].values,\n",
        "    'session_exercise_id': map_session_to_exercise_id,\n",
        "    'session_id': regr_data['session_id'].values\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnHjXkqWqMEV"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a0di4jclkci"
      },
      "source": [
        "### Define Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXAaBR2b7d_Y"
      },
      "outputs": [],
      "source": [
        "def build_model(velocity_scaled,\n",
        "                load_scaled,\n",
        "                session_exercise_id,\n",
        "                session_id,\n",
        "                coords,\n",
        "                render_model = True):\n",
        "    \n",
        "    with pm.Model() as model:\n",
        "\n",
        "        # Add coordinates\n",
        "        model.add_coord('observation', coords['observation'], mutable = True)\n",
        "        model.add_coord('exercise', coords['exercise'], mutable = True)\n",
        "        model.add_coord('session', coords['session'], mutable = True)\n",
        "\n",
        "        # Add inputs\n",
        "        velocity_shared = pm.MutableData('velocity_scaled', velocity_scaled, dims = 'observation')\n",
        "        session_exercise_id = pm.MutableData('session_exercise_id', session_exercise_id, dims = 'session')\n",
        "        session_id = pm.MutableData('session_id', session_id, dims = 'observation')\n",
        "\n",
        "        # Global Parameters\n",
        "        intercept_mu_global = pm.Normal(name = 'intercept_mu_global',\n",
        "                                        mu = 0.0,\n",
        "                                        sigma = 1.0)\n",
        "        \n",
        "        intercept_sigma_global = pm.HalfNormal(name = 'intercept_sigma_global',\n",
        "                                               sigma = 1.0)\n",
        "        \n",
        "        slope_global = pm.HalfNormal(name = 'slope_global',\n",
        "                                     sigma = 1.0)\n",
        "        \n",
        "        curve_global = pm.HalfNormal(name = 'curve_global',\n",
        "                                     sigma = 3.0)\n",
        "        \n",
        "        error_mu_global = pm.HalfNormal(name = 'error_mu_global',\n",
        "                                        sigma = 1.0)\n",
        "        \n",
        "        # Exercise Parameters\n",
        "        intercept_offset_exercise = pm.Normal(name = 'intercept_offset_exercise',\n",
        "                                              mu = 0.0,\n",
        "                                              sigma = 1.0,\n",
        "                                              dims = 'exercise')\n",
        "        \n",
        "        intercept_mu_exercise = pm.Deterministic(name = 'intercept_mu_exercise',\n",
        "                                                 var = intercept_mu_global + intercept_sigma_global*intercept_offset_exercise,\n",
        "                                                 dims = 'exercise')\n",
        "        \n",
        "        intercept_sigma_exercise = pm.HalfNormal(name = 'intercept_sigma_exercise',\n",
        "                                                 sigma = 1.0,\n",
        "                                                 dims = 'exercise')\n",
        "        \n",
        "        slope_exercise = pm.HalfNormal(name = 'slope_exercise',\n",
        "                                       sigma = slope_global,\n",
        "                                       dims = 'exercise')\n",
        "        \n",
        "        curve_exercise = pm.HalfNormal(name = 'curve_exercise',\n",
        "                                       sigma = curve_global,\n",
        "                                       dims = 'exercise')\n",
        "\n",
        "        error_mu_exercise = pm.HalfNormal(name = 'error_mu_exercise',\n",
        "                                          sigma = error_mu_global,\n",
        "                                          dims = 'exercise')\n",
        "        \n",
        "        # Session Parameters\n",
        "        intercept_offset_session = pm.Normal(name = 'intercept_offset_session',\n",
        "                                             mu = 0.0,\n",
        "                                             sigma = 1.0,\n",
        "                                             dims = 'session')\n",
        "\n",
        "        intercept_mu_session = pm.Deterministic(name = 'intercept_mu_session',\n",
        "                                                var = (intercept_mu_exercise[session_exercise_id]\n",
        "                                                    + intercept_sigma_exercise[session_exercise_id]\n",
        "                                                    * intercept_offset_session),\n",
        "                                                dims = 'session')\n",
        "        \n",
        "        slope_session = pm.HalfNormal(name = 'slope_session',\n",
        "                                      sigma = slope_exercise[session_exercise_id],\n",
        "                                      dims = 'session')\n",
        "\n",
        "        curve_session = pm.HalfNormal(name = 'curve_session',\n",
        "                                      sigma = curve_exercise[session_exercise_id],\n",
        "                                      dims = 'session')\n",
        "        \n",
        "        error_mu_session = pm.HalfNormal(name = 'error_mu_session',\n",
        "                                         sigma = error_mu_exercise[session_exercise_id],\n",
        "                                         dims = 'session')\n",
        "\n",
        "        # Final Parameters\n",
        "        intercept = intercept_mu_session[session_id]\n",
        "        slope = slope_session[session_id]\n",
        "        curve = curve_session[session_id]\n",
        "        error = error_mu_session[session_id]\n",
        "\n",
        "        # Estimated Value\n",
        "        load_mu = pm.Deterministic(name = 'load_mu',\n",
        "                                   var = intercept - slope*velocity_shared - curve*velocity_shared**2,\n",
        "                                   dims = 'observation')\n",
        "\n",
        "        # Likelihood\n",
        "        load_likelihood = pm.Normal(name = 'load_scaled',\n",
        "                                    mu = load_mu,\n",
        "                                    sigma = error,\n",
        "                                    observed = load_scaled,\n",
        "                                    dims = 'observation')\n",
        "    \n",
        "    if render_model:\n",
        "        display(pm.model_to_graphviz(model))\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wfAEig5r7sUw"
      },
      "outputs": [],
      "source": [
        "velocity_scaled = regr_data['velocity_scaled']\n",
        "load_scaled = regr_data['load_scaled']\n",
        "session_exercise_id = map_session_to_exercise_id\n",
        "session_id = regr_data['session_id']\n",
        "\n",
        "coords = {'observation': regr_data.index.values,\n",
        "          'exercise': exercise_encoder.classes_,\n",
        "          'session': regr_data['session_id'].unique()}\n",
        "\n",
        "quadratic_model = build_model(velocity_scaled, load_scaled, session_exercise_id, session_id, coords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3o9s9XjmYEs"
      },
      "source": [
        "### Sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8BWLdd-5ClFP"
      },
      "outputs": [],
      "source": [
        "with quadratic_model:\n",
        "    inference_data = pm.sampling_jax.sample_numpyro_nuts(draws = 2000,\n",
        "                                                         tune = 2000,\n",
        "                                                         chains = 2,\n",
        "                                                         #target_accept = 0.98\n",
        "                                                         )\n",
        "    \n",
        "    inference_data.extend(pm.sample_prior_predictive(model = quadratic_model))\n",
        "    inference_data.extend(pm.sample_posterior_predictive(inference_data, model = quadratic_model))\n",
        "\n",
        "inference_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "eNM72GAAEw_g"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    inference_data_path = inference_data.to_netcdf('inference_data.nc')\n",
        "except:\n",
        "    print('Not able to save inference data')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPHzlAd85joA"
      },
      "source": [
        "### Trace Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "kqXuVGp1fO1s"
      },
      "outputs": [],
      "source": [
        "_ = az.plot_trace(inference_data,\n",
        "                  compact = True,\n",
        "                  combined = True,\n",
        "                  figsize = [15, 20],\n",
        "                  var_names = ['intercept_mu_global', 'slope_global', 'curve_global', 'error_mu_global'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "mjC1ot7zHnM7"
      },
      "outputs": [],
      "source": [
        "_ = az.plot_trace(inference_data,\n",
        "                  compact = True,\n",
        "                  combined = True,\n",
        "                  legend = True,\n",
        "                  figsize = [15, 20],\n",
        "                  var_names = ['intercept_mu_exercise', 'slope_exercise', 'curve_exercise'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NPFs-CfuHtUF"
      },
      "outputs": [],
      "source": [
        "_ = az.plot_trace(inference_data,\n",
        "                  compact = True,\n",
        "                  combined = True,\n",
        "                  figsize = [15, 20],\n",
        "                  var_names = ['intercept_mu_session', 'slope_session', 'curve_session'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YK2zkkNcJUS"
      },
      "source": [
        "## Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnLyjybwbGIc"
      },
      "source": [
        "### Generate Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5lMd1J9WQI8"
      },
      "outputs": [],
      "source": [
        "def create_prediction_df(unique_session_df,\n",
        "                         predict_velocity_series):\n",
        "    unique_session_df = unique_session_df.copy()\n",
        "    unique_session_df['_temp'] = True\n",
        "\n",
        "    predict_velocity_df = pd.DataFrame(predict_velocity_series, columns = ['velocity_scaled'])\n",
        "    predict_velocity_df['_temp'] = True\n",
        "\n",
        "    df = (unique_session_df\n",
        "          .merge(predict_velocity_df, on = ['_temp'])\n",
        "          .drop('_temp', axis = 1))\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjTX0B2vFsYz"
      },
      "outputs": [],
      "source": [
        "sessions = regr_data[['exercise', 'workout_start_time', 'exercise_id', 'session_id']].drop_duplicates()\n",
        "\n",
        "velocity_start = 0.0\n",
        "velocity_stop = np.ceil(regr_data['velocity'].max())\n",
        "velocity = np.arange(velocity_start, velocity_stop*100 + 1)/100\n",
        "velocity_scaled = velocity_scaler.transform(velocity.reshape(-1, 1)).flatten()\n",
        "\n",
        "predict_velocities = pd.Series(velocity_scaled)\n",
        "\n",
        "prediction_template = create_prediction_df(sessions, predict_velocities)\n",
        "prediction_template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0r_io7hQ0x7"
      },
      "outputs": [],
      "source": [
        "prediction_template = prediction_template.merge(regr_data[['session_id', 'velocity_scaled', 'load_scaled']],\n",
        "                                                how = 'left',\n",
        "                                                on = ['session_id', 'velocity_scaled'])\n",
        "\n",
        "prediction_template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5CWPite45UE"
      },
      "outputs": [],
      "source": [
        "new_data = {'velocity_scaled': prediction_template['velocity_scaled'].values,\n",
        "            'session_exercise_id': map_session_to_exercise_id,\n",
        "            'session_id': prediction_template['session_id'].values}\n",
        "            \n",
        "new_coords = {'observation': prediction_template.index.values + len(regr_data)}\n",
        "\n",
        "with quadratic_model:\n",
        "    pm.set_data(new_data = new_data,\n",
        "                #coords = new_coords\n",
        "                )\n",
        "    \n",
        "    pm.sample_posterior_predictive(inference_data, predictions = True, extend_inferencedata = True)\n",
        "\n",
        "inference_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2GFyEXc75EE"
      },
      "outputs": [],
      "source": [
        "predictions = inference_data.predictions['load_scaled']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1hDFOPJTqs5"
      },
      "outputs": [],
      "source": [
        "median = predictions.stack({'samples': ['chain', 'draw']}).median('samples')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjjoKOl65IN7"
      },
      "outputs": [],
      "source": [
        "hdi = az.hdi(predictions, hdi_prob = 0.9)['load_scaled']\n",
        "hdi_lower = hdi.sel({'hdi': 'lower'}, drop = True)\n",
        "hdi_upper = hdi.sel({'hdi': 'higher'}, drop = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vi-8EW7loRdx"
      },
      "outputs": [],
      "source": [
        "prediction_df = prediction_template.copy()\n",
        "\n",
        "# Add predictions\n",
        "prediction_df['load_scaled_median'] = median\n",
        "prediction_df['load_scaled_lower'] = hdi_lower\n",
        "prediction_df['load_scaled_upper'] = hdi_upper\n",
        "\n",
        "# Inverse scale all values\n",
        "prediction_df['velocity'] = velocity_scaler.inverse_transform(prediction_df['velocity_scaled'].values.reshape(-1, 1))\n",
        "prediction_df['load'] = load_scaler.inverse_transform(prediction_df['load_scaled'].values.reshape(-1, 1))\n",
        "prediction_df['load_median'] = load_scaler.inverse_transform(prediction_df['load_scaled_median'].values.reshape(-1, 1))\n",
        "prediction_df['load_lower'] = load_scaler.inverse_transform(prediction_df['load_scaled_lower'].values.reshape(-1, 1))\n",
        "prediction_df['load_upper'] = load_scaler.inverse_transform(prediction_df['load_scaled_upper'].values.reshape(-1, 1))\n",
        "\n",
        "prediction_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SM7FWHFkbKqZ"
      },
      "source": [
        "### Plot Last Sessions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "078aLMX6Xn0F"
      },
      "outputs": [],
      "source": [
        "def plot_session(df, session_id, ax):\n",
        "    filter = (df['session_id'] == session_id) & (df['load_median'] >= 0)\n",
        "    data = df[filter]\n",
        "\n",
        "    auc = sklearn.metrics.auc(data['velocity'], data['load_median'])\n",
        "\n",
        "    zero_velocity_load = data['load_median'].max()\n",
        "    zero_velocity_load_lower = data['load_lower'].max()\n",
        "    zero_velocity_load_upper = data['load_upper'].max()\n",
        "\n",
        "    zero_load_velocity = data['velocity'].max()\n",
        "\n",
        "    exercise = data['exercise'].values[0]\n",
        "    workout_time = pd.to_datetime(data['workout_start_time'].values[0]).strftime('%Y-%m-%d')\n",
        "\n",
        "    ax.plot('velocity',\n",
        "            'load',\n",
        "            marker = 'o',\n",
        "            linestyle = '',\n",
        "            label = 'Observed',\n",
        "            zorder = 2,\n",
        "            data = data)\n",
        "    \n",
        "    ax.plot('velocity',\n",
        "            'load_median',\n",
        "            label = 'Median',\n",
        "            zorder = 1,\n",
        "            data = data)\n",
        "\n",
        "    ax.fill_between(x = 'velocity',\n",
        "                    y1 = 'load_lower',\n",
        "                    y2 = 'load_upper',\n",
        "                    alpha = 0.5,\n",
        "                    color = '#ffcd3c',\n",
        "                    label = '90% HDI',\n",
        "                    zorder = 0,\n",
        "                    data = data)\n",
        "\n",
        "    ax.set_title(f'{exercise.title()}\\n{workout_time}',\n",
        "                 fontsize = 'large',\n",
        "                 fontweight = 'bold')\n",
        "    \n",
        "    ax.annotate(f'Zero Velocity Load: {zero_velocity_load:.0f} [{zero_velocity_load_lower:.0f}-{zero_velocity_load_upper:.0f}] kg\\nZero Load Velocity: {zero_load_velocity:.2f} m/s\\nArea Under Curve: {auc:.0f}',\n",
        "                xy = [0.9855, 0.865],\n",
        "                xycoords = 'axes fraction',\n",
        "                horizontalalignment = 'right',\n",
        "                verticalalignment = 'top',\n",
        "                bbox = {'boxstyle': 'round',\n",
        "                        'edgecolor': '#bcbcbc',\n",
        "                        'facecolor': '#eeeeee'})\n",
        "    \n",
        "    ax.legend()\n",
        "    \n",
        "    ax.set_xlabel('Velocity')\n",
        "    ax.set_ylabel('Load')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTexP3bGYmth"
      },
      "outputs": [],
      "source": [
        "exercise_last_session_ids = regr_data.groupby(['exercise_id'])['session_id'].max().values\n",
        "n_exercises = len(exercise_last_session_ids)\n",
        "\n",
        "n_cols = 1 if n_exercises == 1 else 2\n",
        "n_rows = 1 if n_exercises <= 2 else  np.ceil(n_exercises/2).astype(int)\n",
        "\n",
        "fig, axes = plt.subplots(n_cols, n_rows, figsize = (15, 15))\n",
        "\n",
        "axes = np.array([axes])\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i in np.arange(n_exercises):\n",
        "    plot_session(prediction_df, exercise_last_session_ids[i], axes[i])\n",
        "\n",
        "fig.suptitle('LAST SESSION PER EXERCISE',\n",
        "             fontweight = 'bold',\n",
        "             fontsize = 'x-large')\n",
        "\n",
        "fig.tight_layout()\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "DihdfgnGBWuo",
        "l-vn_F0mmmG2",
        "blDhXZLZQ4BB",
        "jXNFV7da7HqZ",
        "VdWQyE4gQ8BF",
        "ubofKRwqoci-",
        "U_PgwhIy7mF0",
        "8G37MnsGlipD",
        "dPHzlAd85joA",
        "4YK2zkkNcJUS"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}